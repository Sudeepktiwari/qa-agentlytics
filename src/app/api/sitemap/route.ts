import { NextRequest, NextResponse } from "next/server";
import { getDb, getAdminSettingsCollection } from "@/lib/mongo";
import {
  verifyApiKey,
  verifyAdminToken,
  verifyAdminTokenFromCookie,
} from "@/lib/auth";
import jwt from "jsonwebtoken";
import fetch from "node-fetch";
import * as cheerio from "cheerio";
import { chunkText } from "@/lib/chunkText";
import {
  addChunks,
  deleteChunksByFilename,
  deleteChunksByUrl,
  querySimilarChunks,
} from "@/lib/chroma";
import {
  snakeTag,
  generateOptionTags,
  generateDiagnosticAnswers,
  processQuestionsWithTags,
  normalizeStructuredSummary,
  enrichStructuredSummary,
} from "@/lib/diagnostic-generation";
import {
  parseSectionBlocks,
  mergeSmallSectionBlocks,
  blocksToSectionedText,
} from "@/lib/parsing";
import { autoGenerateDiagnosticAnswers } from "@/lib/diagnostic";
import { generateStructuredSummaryFromText } from "@/lib/structured-summary";
import OpenAI from "openai";
import { Pinecone } from "@pinecone-database/pinecone";
import puppeteer from "puppeteer";

const JWT_SECRET = process.env.JWT_SECRET || "dev_secret";
const MAX_PAGES = 20;
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// Initialize Pinecone
const pinecone = new Pinecone({
  apiKey: process.env.PINECONE_KEY!,
});
const index = pinecone.index(process.env.PINECONE_INDEX!);

// Helper to check if domains are the same, ignoring www.
function isSameDomain(host1: string, host2: string): boolean {
  const h1 = host1.replace(/^www\./, "").toLowerCase();
  const h2 = host2.replace(/^www\./, "").toLowerCase();
  return h1 === h2 || h1.endsWith("." + h2) || h2.endsWith("." + h1);
}

function buildFallbackStructuredSummaryFromText(text: string) {
  if (!text || typeof text !== "string") return null;
  const sections: any[] = [];
  const sectionRegex =
    /\[SECTION\s+(\d+)\]\s*([^\n]*)\n?([\s\S]*?)(?=(\[SECTION\s+\d+\])|$)/g;
  let match: RegExpExecArray | null;
  while ((match = sectionRegex.exec(text)) !== null) {
    const index = match[1];
    const title = (match[2] || "").trim() || `Section ${index}`;
    const body = (match[3] || "").trim();
    const summary =
      body.length > 400 ? body.slice(0, 400) + "..." : body || title;
    sections.push({
      sectionName: title,
      sectionSummary: summary,
      leadQuestions: [],
      salesQuestions: [],
      scripts: {
        diagnosticAnswer: "",
        followUpQuestion: "",
        followUpOptions: [],
        featureMappingAnswer: "",
        loopClosure: "",
      },
    });
  }
  if (sections.length === 0) {
    const snippet =
      text.length > 400 ? text.slice(0, 400) + "..." : text.trim();
    sections.push({
      sectionName: "Main Content",
      sectionSummary: snippet,
      leadQuestions: [],
      salesQuestions: [],
      scripts: {
        diagnosticAnswer: "",
        followUpQuestion: "",
        followUpOptions: [],
        featureMappingAnswer: "",
        loopClosure: "",
      },
    });
  }
  return {
    pageType: "blog",
    businessVertical: "other",
    primaryFeatures: [],
    painPointsAddressed: [],
    solutions: [],
    targetCustomers: [],
    businessOutcomes: [],
    competitiveAdvantages: [],
    industryTerms: [],
    pricePoints: [],
    integrations: [],
    useCases: [],
    callsToAction: [],
    trustSignals: [],
    sections,
  };
}

function classifySectionType(
  sectionName: string,
  sectionSummary: string,
  sectionText: string,
) {
  const blob = `${sectionName} ${sectionSummary} ${sectionText}`.toLowerCase();
  if (
    /availability|buffer|working hours|multi[-\s]?calendar|slot|rule/i.test(
      blob,
    )
  )
    return "availability";
  if (/secure|security|compliance|admin|governance|privacy|audit/i.test(blob))
    return "security";
  if (
    /roi|return on investment|increase|decrease|bookings|errors|time-to-hire|speed|cycle|customers reached/i.test(
      blob,
    )
  )
    return "roi";
  return "hero";
}

function isGenericLead(sectionName: string, q: any) {
  const base = (sectionName || "").toLowerCase();
  const opts = Array.isArray(q?.options)
    ? q.options.map((o: any) => String(o))
    : [];
  const def =
    opts.length === 3 &&
    opts[0] === "Just exploring" &&
    opts[1] === "Actively evaluating" &&
    opts[2] === "Ready to get started";
  const phr = String(q?.question || "").toLowerCase();
  return def && phr.includes("which best describes your interest");
}

function isGenericSales(sectionName: string, q: any) {
  const opts = Array.isArray(q?.options)
    ? q.options.map((o: any) => String(o))
    : [];
  const def =
    opts.length === 3 &&
    opts[0] === "In the next month" &&
    opts[1] === "In 1-3 months" &&
    opts[2] === "Just researching";
  const flows = Array.isArray(q?.optionFlows) ? q.optionFlows : [];
  return def && flows.length === 0;
}

const TAGS_PROBLEM = [
  "manual_scheduling",
  "scheduling_gap",
  "onboarding_delay",
  "onboarding_dropoff",
  "pipeline_leakage",
  "inconsistent_process",
  "handoff_friction",
  "visibility_gap",
  "no_show_risk",
  "late_engagement",
  "stakeholder_coordination",
  "capacity_constraint",
];

async function refineSectionQuestions(
  openaiClient: any,
  pageUrl: string,
  pageType: string,
  sectionId: string,
  sectionName: string,
  sectionText: string,
  sectionSummary: string,
  sectionType: "hero" | "availability" | "roi" | "security",
) {
  try {
    const resp = await openaiClient.chat.completions.create({
      model: "gpt-4o-mini",
      response_format: { type: "json_object" },
      temperature: 0.2,
      max_tokens: 1500,
      messages: [
        {
          role: "system",
          content: `You are generating Lead and Sales qualification questions for a SaaS page section.

Use ONLY the section-specific data provided:

- section_title
- section_text
- core_keywords
- features
- benefits
- pain_points
- intent_signals
- problem_statement

Your output must follow this EXACT format:

LEAD WORKFLOW — 2 Questions + Options

Goal: Identify visitor intent, motivation, readiness, urgency.

Q1 — Lead Intent / Motivation
[Write a question based on the primary theme or pain point of the section.]

Options
Option 1 — [Visitor motivation aligned with a strong keyword from the section]
→ mapping tag

Option 2 — [Another strong motivation aligned with a different insight]
→ mapping tag

Option 3 — [Exploratory or low-intent option]
→ mapping tag

Option 4 — [“Just browsing / early stage” option]
→ mapping tag

Q2 — Secondary Intent / Urgency / Related Need
IMPORTANT: Q2 must NOT repeat Q1.
It must use a DIFFERENT keyword, feature, pain point, or benefit from the section.

Options
(Use the same logic as Q1, but tied to the new keyword/theme.)

---

SALES WORKFLOW — 2 Questions + Options

Goal: Understand sophistication, current workflows, replaceability, and desired outcomes.

Q1 — Current Process (Problem-Aware)
[Ask about the specific current workflow or problem identified in the 'problem_statement' and 'core_keywords'.]
[The question must reference the specific domain topic of this section.]

Options (4)
- [Specific manual/inefficient method from section text]
- [Specific basic/partial solution from section text]
- [Specific lack of solution/process from section text]
- [Specific alternative/competitor approach from section text]
(Map each using the awareness + urgency logic.)

Q2 — Desired Outcome / Improvement
Based on a DIFFERENT 'benefit' or 'feature' keyword from Q1.
[Ask what they want to achieve regarding the specific section topic.]

Options (4)
- [Specific high-value outcome mentioned in section benefits] → sales_alert
- [Specific optimization/efficiency outcome] → validation_path
- [Specific learning/curiosity outcome] → diagnostic_education
- [Specific uncertainty/researching outcome] → diagnostic_education

---

RULES
- Do NOT mention the page URL.
- Do NOT create generic questions. Every question must be grounded in actual keywords from this section.
- Do NOT use generic option labels like "Manual process" or "No process". Make them specific to the domain (e.g. "Using spreadsheets", "Calling manually").
- Q2 must always use a different theme from Q1.
- Output JSON ONLY with structure: { "leadQuestions": [], "salesQuestions": [] }
- Each question object: { "question": "", "options": [{ "label": "", "tags": [], "workflow": "" }] }
- Mapping Logic:
  - awarenesspresent, optimizationready → validation_path
  - awarenesspresent, mediumintent → validation_path
  - awarenesspresent, highintent → sales_alert
  - unknownstate, lowrisk → diagnostic_education
  - awarenessmissing, lowrisk → diagnostic_education
`,
        },
        {
          role: "user",
          content: `Analyze this section and generate the configuration.

First, extract these elements from the text below:
- core_keywords
- features
- benefits
- pain_points
- intent_signals
- problem_statement

SECTION DATA:
page_url: ${pageUrl}
page_type: ${pageType}
section_id: ${sectionId}
section_heading: ${sectionName}
section_summary: ${sectionSummary}
section_type: ${sectionType}
section_text: "${sectionText}"

Generate the JSON response.
`,
        },
      ],
    });
    const txt = resp.choices[0]?.message?.content || "";
    const data = JSON.parse(txt);
    return data;
  } catch (error) {
    console.error(
      `[refineSectionQuestions] Error generating questions for section "${sectionName}":`,
      error,
    );
    return null;
  }
}

// Auto-extract personas after crawling is complete
import { generateBantFromContent } from "@/lib/bant-generation";

async function autoGenerateBantConfig(adminId: string) {
  try {
    const db = await getDb();
    const crawledPages = db.collection("crawled_pages");

    // Get crawled content for this admin
    const pages = await crawledPages
      .find({ adminId })
      .sort({ created_at: -1 }) // Get most recent pages
      .limit(10)
      .toArray();

    if (pages.length === 0) {
      // console.log removed
      return;
    }

    const context = pages
      .map(
        (p) =>
          `URL: ${p.url}\nTitle: ${p.title}\nContent Summary: ${p.text.substring(0, 500)}...`,
      )
      .join("\n\n");

    const newConfig = await generateBantFromContent(adminId, context);

    // Save to DB
    const bantCollection = db.collection("bant_configurations");
    await bantCollection.updateOne(
      { adminId },
      { $set: newConfig },
      { upsert: true },
    );
    // console.log removed
  } catch (error) {
    // console.error removed
  }
}

async function extractPersonasForAdmin(adminId: string, websiteUrl: string) {
  try {
    const db = await getDb();
    const crawledPages = db.collection("crawled_pages");
    const personas = db.collection("customer_personas");

    // Get crawled content for this admin
    const pages = await crawledPages
      .find({ adminId })
      .limit(20) // Limit to prevent token overflow
      .toArray();

    const websiteContent = pages.map((page) => page.text || "").filter(Boolean);

    if (websiteContent.length === 0) {
      // console.log removed
      return;
    }

    const prompt = `
Analyze this website content and extract detailed customer persona data. Focus on identifying who the target customers are, their characteristics, and buying patterns.

Website URL: ${websiteUrl}
Content: ${websiteContent.slice(0, 10).join("\n---\n")}

Extract and return a JSON object with this structure:
{
  "websiteUrl": "${websiteUrl}",
  "targetAudiences": [
    {
      "id": "unique_id",
      "name": "Persona Name",
      "type": "small_business|enterprise|startup|freelancer|agency",
      "industries": ["general"],
      "companySize": "1-10|11-50|51-200|200+",
      "painPoints": ["pain point 1", "pain point 2"],
      "preferredFeatures": ["feature1", "feature2"],
      "buyingPatterns": ["pattern1", "pattern2"],
      "budget": "under_500|500_2000|2000_10000|10000_plus",
      "technicalLevel": "beginner|intermediate|advanced",
      "urgency": "low|medium|high",
      "decisionMaker": true|false,
      "bantQuestions": {
        "budget": [
          { "question": "What is your expected monthly budget?", "options": ["Under $500", "$500-$2k", "$2k-$5k", "$5k+"] }
        ],
        "authority": [
          { "question": "Are you the final decision maker?", "options": ["Yes, I decide", "I need approval", "I'm researching"] }
        ],
        "need": [
          { "question": "What is your biggest challenge right now?", "options": ["High costs", "Low efficiency", "Compliance", "Other"] }
        ],
        "timeline": [
          { "question": "When are you looking to implement this?", "options": ["Immediately", "1-3 months", "3-6 months", "Just looking"] }
        ]
      }
    }
  ],
  "industryFocus": ["primary industries served"],
  "useCaseExamples": ["use case 1", "use case 2"],
  "competitorMentions": ["competitor1", "competitor2"],
  "pricingStrategy": "freemium|subscription|one_time|custom"
}

Guidelines:
- Create 2-4 distinct personas based on the content
- Be specific about pain points and preferred features
- Identify clear buying patterns and budget ranges
- Look for mentions of company sizes and use cases
- For "industries" field, use ["general"] unless the website is clearly industry-specific (e.g., a dental practice website). Do NOT assume specific industries from generic business content.
- Extract actual competitor names mentioned
- Determine pricing strategy from pricing pages or content
- Each persona should be distinct and actionable for messaging
- For "bantQuestions", generate 2-3 specific, relevant qualification questions for EACH category (Budget, Authority, Need, Timeline).
  - For EACH question, provide 3-4 "options" that the user can click as quick replies.
  - Budget questions should match their likely financial scale.
  - Authority questions should respect their role.
  - Need questions should probe their specific pain points.
  - Timeline questions should relate to their urgency level.
`;

    const completion = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [
        {
          role: "system",
          content:
            "You are a customer persona analyst. Extract detailed, actionable customer personas from website content. Always return valid JSON.",
        },
        { role: "user", content: prompt },
      ],
      temperature: 0.3,
    });

    const extracted = JSON.parse(completion.choices[0].message.content || "{}");

    // Add timestamps and IDs to personas
    extracted.targetAudiences = extracted.targetAudiences.map(
      (persona: any, index: number) => ({
        ...persona,
        id: persona.id || `persona_${index + 1}`,
        createdAt: new Date(),
        updatedAt: new Date(),
      }),
    );

    const personaDocument = {
      adminId,
      ...extracted,
      extractedAt: new Date(),
      updatedAt: new Date(),
    };

    await personas.replaceOne({ adminId }, personaDocument, { upsert: true });

    // console.log removed
  } catch (error) {
    // console.error removed
    throw error;
  }
}

async function parseSitemap(
  sitemapUrl: string,
  depth: number = 0,
): Promise<string[]> {
  if (depth > 3) {
    // console.log removed
    return [];
  }

  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), 60000);

  try {
    const res = await fetch(sitemapUrl, {
      signal: controller.signal,
      headers: {
        "User-Agent":
          "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        Accept:
          "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "Accept-Language": "en-US,en;q=0.9",
        "Cache-Control": "no-cache",
        Pragma: "no-cache",
      },
    });
    clearTimeout(timeoutId);

    if (!res.ok) throw new Error("Failed to fetch sitemap");
    const xml = await res.text();

    // console.log removed

    const $ = cheerio.load(xml, { xmlMode: true });
    const urls: string[] = [];

    // Check for sitemap index (recursive)
    const childSitemaps = $("sitemap > loc");
    if (childSitemaps.length > 0) {
      // console.log removed
      const sitemapUrls: string[] = [];
      childSitemaps.each((_, el) => {
        const loc = $(el).text().trim();
        if (loc) sitemapUrls.push(loc);
      });

      // Process sub-sitemaps sequentially to avoid overwhelming
      for (const url of sitemapUrls) {
        try {
          const nestedUrls = await parseSitemap(url, depth + 1);
          urls.push(...nestedUrls);
        } catch (err) {
          // console.error removed
        }
      }
    }

    // Check for standard URL set
    const pageLocs = $("url > loc");
    if (pageLocs.length > 0) {
      // console.log removed
      pageLocs.each((_, el) => {
        const loc = $(el).text().trim();
        if (loc) urls.push(loc);
      });
    }

    // Fallback: If specific tags not found, try generic regex as last resort
    // (This helps with malformed XML or unusual namespaces)
    if (urls.length === 0) {
      // console.log removed
      const matches = xml.matchAll(/<loc>(.*?)<\/loc>/g);
      for (const match of matches) {
        urls.push(match[1].trim());
      }
    }

    // console.log removed
    return urls;
  } catch (error) {
    clearTimeout(timeoutId);
    if (error instanceof Error && error.name === "AbortError") {
      throw new Error("Sitemap fetch timed out after 30 seconds");
    }
    throw error;
  }
}

async function discoverSitemapCandidates(inputUrl: string): Promise<string[]> {
  const candidates = new Set<string>();
  let origin = "";
  try {
    const u = new URL(inputUrl);
    origin = `${u.protocol}//${u.hostname}`;
  } catch {
    return [];
  }
  candidates.add(`${origin}/sitemap.xml`);
  candidates.add(`${origin}/sitemap_index.xml`);
  candidates.add(`${origin}/hc/sitemap.xml`);
  candidates.add(`${origin}/help/sitemap.xml`);
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 15000);
    const res = await fetch(`${origin}/robots.txt`, {
      signal: controller.signal,
      headers: {
        "User-Agent":
          "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        Accept: "text/plain,text/html,*/*",
      },
    });
    clearTimeout(timeoutId);
    if (res.ok) {
      const text = await res.text();
      const lines = text.split("\n");
      for (const line of lines) {
        const m = line.match(/sitemap:\s*(\S+)/i);
        if (m && m[1]) {
          try {
            const sUrl = new URL(m[1], origin).href;
            candidates.add(sUrl);
          } catch {}
        }
      }
    }
  } catch {}
  return Array.from(candidates);
}

async function extractLinksUsingBrowser(
  pageUrl: string,
  adminId?: string | null,
): Promise<string[]> {
  // console.log removed

  let browser;
  try {
    browser = await puppeteer.launch({
      headless: true,
      args: [
        "--no-sandbox",
        "--disable-setuid-sandbox",
        "--disable-dev-shm-usage",
        "--disable-accelerated-2d-canvas",
        "--no-first-run",
        "--no-zygote",
        "--disable-gpu",
      ],
    });

    const page = await browser.newPage();

    // Set a reasonable viewport and user agent
    await page.setViewport({ width: 1280, height: 720 });
    await page.setUserAgent(
      "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    );

    // Navigate to the page with timeout
    // console.log removed
    await page.goto(pageUrl, {
      waitUntil: "networkidle2", // Wait until network is mostly idle
      timeout: 30000,
    });

    // Wait a bit more for any dynamic content to load
    // console.log removed
    await new Promise((resolve) => setTimeout(resolve, 3000)); // Initial wait

    // Check if page has loaded properly
    const pageTitle = await page.title();
    // console.log removed

    // Handle infinite scrolling by scrolling down multiple times
    // console.log removed
    let previousLinkCount = 0;
    let scrollAttempts = 0;
    const maxScrollAttempts = 15; // Increased attempts for more thorough scrolling

    while (scrollAttempts < maxScrollAttempts) {
      // Check for stop signal
      if (adminId) {
        const db = await getDb();
        const state = await db.collection("crawl_states").findOne({ adminId });
        if (state?.status === "stopped") {
          // console.log removed
          break;
        }
      }

      // Get current link count and content-specific count
      const { currentLinkCount, currentContentCount } = await page.evaluate(
        () => {
          const allLinks = document.querySelectorAll("a[href]").length;

          // Intelligent content detection in browser
          const contentPatterns = [
            /\/blog\//i,
            /\/post\//i,
            /\/article\//i,
            /\/slide\//i,
            /\/news\//i,
            /\/help\//i,
            /\/guide\//i,
            /\/tutorial\//i,
            /\/docs?\//i,
            /\/support\//i,
            /\/resource\//i,
            /\/case-stud/i,
            /\/faq\//i,
          ];

          const contentLinks = Array.from(
            document.querySelectorAll("a[href]"),
          ).filter((el) => {
            const href = el.getAttribute("href");
            return (
              href && contentPatterns.some((pattern) => pattern.test(href))
            );
          }).length;

          return {
            currentLinkCount: allLinks,
            currentContentCount: contentLinks,
          };
        },
      );

      // console.log removed

      // If no new links were loaded after scrolling, try a few more times
      if (scrollAttempts > 2 && currentLinkCount === previousLinkCount) {
        // console.log removed
        // Try scrolling more aggressively
        await page.evaluate(() => {
          window.scrollTo(0, document.body.scrollHeight + 1000);
        });
        await new Promise((resolve) => setTimeout(resolve, 3000));

        const { finalLinkCount } = await page.evaluate(() => {
          return {
            finalLinkCount: document.querySelectorAll("a[href]").length,
          };
        });

        if (finalLinkCount === currentLinkCount) {
          // console.log removed
          break;
        }
      }

      previousLinkCount = currentLinkCount;

      // Scroll to bottom of page with multiple strategies
      await page.evaluate(() => {
        // Strategy 1: Scroll to bottom
        window.scrollTo(0, document.body.scrollHeight);

        // Strategy 2: Also try scrolling the document element
        if (
          document.documentElement.scrollHeight > document.body.scrollHeight
        ) {
          window.scrollTo(0, document.documentElement.scrollHeight);
        }

        // Strategy 3: Smooth scroll to trigger lazy loading
        window.scrollTo({
          top: document.body.scrollHeight,
          behavior: "smooth",
        });

        // Strategy 4: Trigger scroll events that might activate infinite scroll
        window.dispatchEvent(new Event("scroll"));
        document.dispatchEvent(new Event("scroll"));
      });

      // Wait for new content to load with progressive waiting
      // console.log removed
      await new Promise((resolve) => setTimeout(resolve, 2000)); // Initial wait

      // Check if there are loading indicators and wait longer if needed
      const hasLoadingIndicators = await page.evaluate(() => {
        const loadingSelectors = [
          '[class*="loading"]',
          '[class*="spinner"]',
          '[class*="loader"]',
          ".loading",
          ".spinner",
          ".loader",
        ];

        return loadingSelectors.some((selector) => {
          const elements = document.querySelectorAll(selector);
          return Array.from(elements).some((el) => {
            const htmlEl = el as HTMLElement;
            return htmlEl.offsetHeight > 0 && htmlEl.offsetWidth > 0; // Element is visible
          });
        });
      });

      if (hasLoadingIndicators) {
        // console.log removed
        await new Promise((resolve) => setTimeout(resolve, 4000));
      } else {
        await new Promise((resolve) => setTimeout(resolve, 1000));
      }

      scrollAttempts++;
    }

    // console.log removed

    // Get final counts after all scrolling
    const finalCounts = await page.evaluate(() => {
      const allLinks = document.querySelectorAll("a[href]").length;

      // Use the same intelligent content detection
      const contentPatterns = [
        /\/blog\//i,
        /\/post\//i,
        /\/article\//i,
        /\/slide\//i,
        /\/news\//i,
        /\/help\//i,
        /\/guide\//i,
        /\/tutorial\//i,
        /\/docs?\//i,
        /\/support\//i,
        /\/resource\//i,
        /\/case-stud/i,
        /\/faq\//i,
      ];

      const contentLinks = Array.from(
        document.querySelectorAll("a[href]"),
      ).filter((el) => {
        const href = el.getAttribute("href");
        return href && contentPatterns.some((pattern) => pattern.test(href));
      }).length;

      return { totalLinks: allLinks, contentLinks };
    });

    // console.log removed

    // Extract all links from the rendered page with enhanced deduplication
    const links = await page.evaluate((currentUrl) => {
      const linkElements = document.querySelectorAll("a[href]");
      const foundLinks = new Set<string>();
      const processedHrefs = new Set<string>(); // Track processed hrefs to avoid duplicates

      // console.log removed

      // Add the current page
      foundLinks.add(currentUrl);

      linkElements.forEach((element, index) => {
        const href = element.getAttribute("href");
        if (href && !processedHrefs.has(href)) {
          // Skip if we've already processed this href
          processedHrefs.add(href);

          try {
            // Convert relative URLs to absolute
            const absoluteUrl = new URL(href, currentUrl).href;
            const linkUrl = new URL(absoluteUrl);
            const pageUrlObj = new URL(currentUrl);

            // Only include same-domain HTTP/HTTPS URLs
            const h1 = linkUrl.hostname.replace(/^www\./, "").toLowerCase();
            const h2 = pageUrlObj.hostname.replace(/^www\./, "").toLowerCase();

            if (linkUrl.protocol.startsWith("http") && h1 === h2) {
              // Clean up the URL
              let cleanUrl = absoluteUrl.split("#")[0];

              // Remove tracking parameters
              const url = new URL(cleanUrl);
              const paramsToRemove = [
                "utm_source",
                "utm_medium",
                "utm_campaign",
                "utm_term",
                "utm_content",
                "ref",
                "source",
              ];
              paramsToRemove.forEach((param) => url.searchParams.delete(param));
              cleanUrl = url.toString();

              // Skip file extensions we can't crawl
              const extension = cleanUrl.split(".").pop()?.toLowerCase();
              const skipExtensions = [
                "pdf",
                "doc",
                "docx",
                "jpg",
                "jpeg",
                "png",
                "gif",
                "svg",
                "mp4",
                "mp3",
                "css",
                "js",
                "ico",
              ];

              const hasSkipExtension =
                extension && skipExtensions.includes(extension);

              // Skip common non-content URLs
              const skipPatterns = [
                "/wp-admin/",
                "/admin/",
                "/login",
                "/register/",
                "/contact",
                "/privacy",
                "/terms",
                "mailto:",
                "tel:",
              ];
              const hasSkipPattern = skipPatterns.some((pattern) =>
                cleanUrl.includes(pattern),
              );

              if (
                !hasSkipExtension &&
                !hasSkipPattern &&
                cleanUrl !== currentUrl &&
                !foundLinks.has(cleanUrl) // Additional check to prevent duplicates
              ) {
                foundLinks.add(cleanUrl);

                // Log content-related links as we find them (but only first few to avoid spam)
                const contentPatterns = [
                  /\/blog\//i,
                  /\/post\//i,
                  /\/article\//i,
                  /\/slide\//i,
                  /\/news\//i,
                  /\/help\//i,
                  /\/guide\//i,
                  /\/tutorial\//i,
                  /\/docs?\//i,
                  /\/support\//i,
                  /\/resource\//i,
                  /\/case-stud/i,
                  /\/faq\//i,
                ];

                const matchedPattern = contentPatterns.find((pattern) =>
                  pattern.test(cleanUrl),
                );
                if (matchedPattern && index < 20) {
                  // Only log first 20 content links to avoid spam
                  const linkType = cleanUrl.includes("/slide")
                    ? "slide"
                    : cleanUrl.includes("/blog")
                      ? "blog"
                      : cleanUrl.includes("/post")
                        ? "post"
                        : cleanUrl.includes("/article")
                          ? "article"
                          : cleanUrl.includes("/help")
                            ? "help"
                            : cleanUrl.includes("/guide")
                              ? "guide"
                              : cleanUrl.includes("/news")
                                ? "news"
                                : cleanUrl.includes("/tutorial")
                                  ? "tutorial"
                                  : cleanUrl.includes("/docs")
                                    ? "docs"
                                    : cleanUrl.includes("/support")
                                      ? "support"
                                      : cleanUrl.includes("/resource")
                                        ? "resource"
                                        : cleanUrl.includes("/case-stud")
                                          ? "case-study"
                                          : cleanUrl.includes("/faq")
                                            ? "faq"
                                            : "content";
                  // console.log removed
                }
              }
            }
          } catch {
            // Skip invalid URLs
            if (index < 10) {
              // Only log first 10 invalid URLs to avoid spam
              // console.log removed
            }
          }
        }
      });

      const allLinks = Array.from(foundLinks);

      // Use intelligent content detection for final summary
      const contentPatterns = [
        { pattern: /\/blog\//i, name: "blog" },
        { pattern: /\/post\//i, name: "post" },
        { pattern: /\/article\//i, name: "article" },
        { pattern: /\/slide\//i, name: "slide" },
        { pattern: /\/news\//i, name: "news" },
        { pattern: /\/help\//i, name: "help" },
        { pattern: /\/guide\//i, name: "guide" },
        { pattern: /\/tutorial\//i, name: "tutorial" },
        { pattern: /\/docs?\//i, name: "docs" },
        { pattern: /\/support\//i, name: "support" },
        { pattern: /\/resource\//i, name: "resource" },
        { pattern: /\/case-stud/i, name: "case-study" },
        { pattern: /\/faq\//i, name: "faq" },
      ];

      const contentLinks = allLinks.filter((url) =>
        contentPatterns.some((cp) => cp.pattern.test(url)),
      );

      // console.log removed

      // Break down by content type
      const contentBreakdown: Record<string, number> = {};
      contentPatterns.forEach(({ pattern, name }) => {
        const count = allLinks.filter((url) => pattern.test(url)).length;
        if (count > 0) {
          contentBreakdown[name] = count;
        }
      });

      if (Object.keys(contentBreakdown).length > 0) {
        // console.log removed
      }

      return allLinks;
    }, pageUrl);

    // console.log removed

    // Use intelligent content analysis for final summary
    const finalAnalysis = analyzeUrlPatterns(links, pageUrl);
    // console.log removed

    return links;
  } catch (error) {
    // console.log removed
    throw error;
  } finally {
    if (browser) {
      await browser.close();
    }
  }
}

async function extractLinksFromPage(pageUrl: string): Promise<string[]> {
  try {
    const res = await fetch(pageUrl, {
      headers: {
        "User-Agent":
          "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        Accept:
          "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate",
        Connection: "keep-alive",
        "Upgrade-Insecure-Requests": "1",
      },
      redirect: "follow",
    });

    // console.log removed

    if (!res.ok) {
      throw new Error(
        `Failed to fetch page: ${pageUrl} (Status: ${res.status} ${res.statusText})`,
      );
    }

    // Use final URL after redirects for base resolution
    const finalUrl = res.url;
    // console.log removed

    const html = await res.text();
    const $ = cheerio.load(html);

    const links = new Set<string>();

    // Add the original page itself (and the final URL if different)
    links.add(pageUrl);
    if (finalUrl !== pageUrl) {
      links.add(finalUrl);
    }

    // Extract all links from the page with more comprehensive selectors
    $("a[href]").each((_, element) => {
      const href = $(element).attr("href");
      if (href) {
        try {
          // Convert relative URLs to absolute using final URL
          const absoluteUrl = new URL(href, finalUrl).href;

          // Only include HTTP/HTTPS URLs from the same domain
          const pageUrlObj = new URL(finalUrl);
          const linkUrlObj = new URL(absoluteUrl);

          if (
            linkUrlObj.protocol.startsWith("http") &&
            isSameDomain(linkUrlObj.hostname, pageUrlObj.hostname)
          ) {
            // Remove fragments and query parameters for cleaner URLs
            let cleanUrl = absoluteUrl.split("#")[0];
            // Remove common tracking parameters but keep important query params
            const url = new URL(cleanUrl);
            const paramsToRemove = [
              "utm_source",
              "utm_medium",
              "utm_campaign",
              "utm_term",
              "utm_content",
              "ref",
              "source",
            ];
            paramsToRemove.forEach((param) => url.searchParams.delete(param));
            cleanUrl = url.toString();

            const extension = cleanUrl.split(".").pop()?.toLowerCase();
            const skipExtensions = [
              "pdf",
              "doc",
              "docx",
              "xls",
              "xlsx",
              "ppt",
              "pptx",
              "zip",
              "rar",
              "exe",
              "dmg",
              "jpg",
              "jpeg",
              "png",
              "gif",
              "svg",
              "mp4",
              "mp3",
              "avi",
              "mov",
              "css",
              "js",
              "ico",
              "woff",
              "woff2",
              "ttf",
              "eot",
            ];

            // Skip URLs that end with file extensions we can't crawl
            const hasSkipExtension =
              extension && skipExtensions.includes(extension);

            // Skip common non-content URLs
            const skipPatterns = [
              "/wp-admin/",
              "/admin/",
              "/login",
              "/register",
              "/contact",
              "/privacy",
              "/terms",
              "/sitemap",
              "mailto:",
              "tel:",
              "#",
            ];

            const hasSkipPattern = skipPatterns.some((pattern) =>
              cleanUrl.includes(pattern),
            );

            if (!hasSkipExtension && !hasSkipPattern && cleanUrl !== pageUrl) {
              links.add(cleanUrl);
              // console.log removed
            }
          }
        } catch {
          // Skip invalid URLs
          // console.log removed
        }
      }
    });

    const pageLinks = Array.from(links);

    // console.log removed

    const intelligentAnalysis = analyzeUrlPatterns(pageLinks, pageUrl);
    // console.log removed

    return pageLinks;
  } catch (error) {
    // console.log removed
    throw error;
  }
}

// Intelligent URL pattern analysis
function analyzeUrlPatterns(urls: string[], inputUrl: string) {
  // Common content patterns to look for (dynamically extensible)
  const knownContentPatterns = [
    { pattern: /\/blog\//i, name: "blog", weight: 1.0 },
    { pattern: /\/post\//i, name: "post", weight: 1.0 },
    { pattern: /\/article\//i, name: "article", weight: 1.0 },
    { pattern: /\/slide\//i, name: "slide", weight: 1.0 },
    { pattern: /\/news\//i, name: "news", weight: 0.9 },
    { pattern: /\/help\//i, name: "help", weight: 0.8 },
    { pattern: /\/guide\//i, name: "guide", weight: 0.8 },
    { pattern: /\/tutorial\//i, name: "tutorial", weight: 0.8 },
    { pattern: /\/docs\//i, name: "docs", weight: 0.7 },
    { pattern: /\/support\//i, name: "support", weight: 0.7 },
    { pattern: /\/faq\//i, name: "faq", weight: 0.6 },
    { pattern: /\/case-stud/i, name: "case-study", weight: 0.8 },
    { pattern: /\/resource\//i, name: "resource", weight: 0.7 },
  ];

  const detectedPatterns: Array<{
    name: string;
    count: number;
    weight: number;
    urls: string[];
  }> = [];
  const contentUrls: string[] = [];
  const patternMap = new Map<string, string[]>();

  // Analyze each URL against known patterns
  urls.forEach((url) => {
    knownContentPatterns.forEach(({ pattern, name }) => {
      if (pattern.test(url)) {
        if (!patternMap.has(name)) {
          patternMap.set(name, []);
        }
        patternMap.get(name)!.push(url);
        if (!contentUrls.includes(url)) {
          contentUrls.push(url);
        }
      }
    });
  });

  // Build detected patterns summary
  patternMap.forEach((urls, name) => {
    const patternInfo = knownContentPatterns.find((p) => p.name === name);
    if (patternInfo) {
      detectedPatterns.push({
        name,
        count: urls.length,
        weight: patternInfo.weight,
        urls: urls.slice(0, 5), // Sample URLs
      });
    }
  });

  // Sort by relevance (count * weight)
  detectedPatterns.sort((a, b) => b.count * b.weight - a.count * a.weight);

  // Detect URL path depth patterns (for dynamic content detection)
  const pathAnalysis = analyzePathDepth(urls, inputUrl);

  return {
    contentUrls,
    detectedPatterns,
    pathAnalysis,
    totalContentScore: detectedPatterns.reduce(
      (sum, p) => sum + p.count * p.weight,
      0,
    ),
  };
}

function analyzePathDepth(urls: string[], inputUrl: string) {
  try {
    const inputPath = new URL(inputUrl).pathname;
    const inputDepth = inputPath
      .split("/")
      .filter((segment) => segment.length > 0).length;

    const pathDepths = urls.map((url) => {
      try {
        const path = new URL(url).pathname;
        return path.split("/").filter((segment) => segment.length > 0).length;
      } catch {
        return 0;
      }
    });

    const avgDepth =
      pathDepths.reduce((sum, depth) => sum + depth, 0) / pathDepths.length;
    const maxDepth = Math.max(...pathDepths);
    const minDepth = Math.min(...pathDepths.filter((d) => d > 0));

    return {
      inputDepth,
      avgDepth,
      maxDepth,
      minDepth,
      hasDeepPaths: maxDepth > inputDepth + 1, // URLs go deeper than the listing page
      depthVariation: maxDepth - minDepth,
    };
  } catch {
    return {
      inputDepth: 0,
      avgDepth: 0,
      maxDepth: 0,
      minDepth: 0,
      hasDeepPaths: false,
      depthVariation: 0,
    };
  }
}

interface UrlAnalysis {
  contentUrls: string[];
  detectedPatterns: Array<{
    name: string;
    count: number;
    weight: number;
    urls: string[];
  }>;
  pathAnalysis: {
    hasDeepPaths: boolean;
  };
}

function detectDynamicContentPage(
  inputUrl: string,
  urlAnalysis: UrlAnalysis,
  totalUrls: number,
) {
  const hasMinimalLinks = totalUrls <= 10;
  const hasMinimalContent = urlAnalysis.contentUrls.length <= 3;
  const hasZeroContent = urlAnalysis.contentUrls.length === 0;

  // Check if URL looks like a listing page (plural form or common listing patterns)
  const listingPatterns = [
    /\/blogs?\/?$/i,
    /\/posts?\/?$/i,
    /\/articles?\/?$/i,
    /\/slides?\/?$/i,
    /\/news\/?$/i,
    /\/help\/?$/i,
    /\/guides?\/?$/i,
    /\/tutorials?\/?$/i,
    /\/docs?\/?$/i,
    /\/support\/?$/i,
    /\/resources?\/?$/i,
    /\/case-studies?\/?$/i,
    /\/faqs?\/?$/i,
  ];

  const isListingPage = listingPatterns.some((pattern) =>
    pattern.test(inputUrl),
  );

  // Check if URL contains any content-related keywords
  const hasContentKeywords =
    urlAnalysis.detectedPatterns.length > 0 ||
    /\/(blog|post|article|slide|news|help|guide|tutorial|doc|support|resource|case-stud|faq)/i.test(
      inputUrl,
    );

  // Determine if this looks like a dynamic content page
  const shouldUseJavaScript =
    // Case 0: Almost no links found (likely SPA/JS-rendered) - increased threshold
    totalUrls < 15 ||
    // Case 1: It's clearly a listing page
    isListingPage ||
    // Case 2: Has content keywords but found very few/no content URLs (likely dynamic)
    (hasContentKeywords && (hasMinimalContent || hasZeroContent)) ||
    // Case 3: Very few total links found (might be dynamic loading)
    (hasContentKeywords && hasMinimalLinks) ||
    // Case 4: URL suggests content but we found no deeper paths (might load dynamically)
    (hasContentKeywords && !urlAnalysis.pathAnalysis.hasDeepPaths);

  return {
    shouldUseJavaScript,
    reasons: {
      isListingPage,
      hasContentKeywords,
      hasMinimalLinks,
      hasMinimalContent,
      hasZeroContent,
      lacksDeepPaths:
        hasContentKeywords && !urlAnalysis.pathAnalysis.hasDeepPaths,
    },
    confidence: calculateConfidence(
      isListingPage,
      hasContentKeywords,
      hasMinimalContent,
      hasZeroContent,
      hasMinimalLinks,
    ),
  };
}

function calculateConfidence(
  isListingPage: boolean,
  hasContentKeywords: boolean,
  hasMinimalContent: boolean,
  hasZeroContent: boolean,
  hasMinimalLinks: boolean,
): number {
  let confidence = 0;

  if (isListingPage) confidence += 0.4; // Strong indicator
  if (hasContentKeywords) confidence += 0.2;
  if (hasZeroContent && hasContentKeywords) confidence += 0.3; // Very likely dynamic
  if (hasMinimalContent && hasContentKeywords) confidence += 0.2;
  if (hasMinimalLinks && hasContentKeywords) confidence += 0.1;

  return Math.min(confidence, 1.0); // Cap at 1.0
}

async function discoverUrls(
  inputUrl: string,
  adminId?: string | null,
): Promise<{ urls: string[]; type: "sitemap" | "webpage" | "javascript" }> {
  // console.log removed

  // First, try to parse as sitemap
  try {
    const urls = await parseSitemap(inputUrl);
    if (urls.length > 0) {
      // console.log removed
      // console.log removed
      return { urls, type: "sitemap" };
    }
  } catch (error) {
    // console.log removed
  }

  // Try common sitemap candidates from robots.txt and known paths
  try {
    const candidates = await discoverSitemapCandidates(inputUrl);
    if (candidates.length > 0) {
      // console.log removed
      for (const candidate of candidates) {
        try {
          const urls = await parseSitemap(candidate);
          if (urls.length > 0) {
            // console.log removed
            return { urls, type: "sitemap" };
          }
        } catch {}
      }
    }
  } catch {}

  // If sitemap parsing fails, try regular HTML crawling first
  try {
    const urls = await extractLinksFromPage(inputUrl);
    // console.log removed

    // Intelligent content detection - analyze URL patterns dynamically
    const urlAnalysis = analyzeUrlPatterns(urls, inputUrl);

    // console.log removed

    const totalUrls = urls.length;
    const contentUrls = urlAnalysis.contentUrls.length;

    // Intelligent detection: check if this looks like a dynamic content page
    const isDynamicContentPage = detectDynamicContentPage(
      inputUrl,
      urlAnalysis,
      totalUrls,
    );

    // console.log removed

    if (isDynamicContentPage.shouldUseJavaScript) {
      // console.log removed

      try {
        const jsUrls = await extractLinksUsingBrowser(inputUrl, adminId);
        const jsUrlAnalysis = analyzeUrlPatterns(jsUrls, inputUrl);
        const jsContentUrls = jsUrlAnalysis.contentUrls.length;

        // console.log removed

        // If JavaScript rendering found more content URLs, use those results
        if (jsUrls.length > totalUrls || jsContentUrls > contentUrls) {
          // console.log removed

          // Ensure no duplicates in the final result
          const uniqueJsUrls = Array.from(new Set(jsUrls));
          // console.log removed

          return { urls: uniqueJsUrls, type: "javascript" };
        }
      } catch (jsError) {
        // console.log removed
      }
    }

    // console.log removed

    // Ensure no duplicates in regular webpage results either
    const uniqueUrls = Array.from(new Set(urls));
    if (uniqueUrls.length !== urls.length) {
      // console.log removed
    }

    return { urls: uniqueUrls, type: "webpage" };
  } catch (error) {
    // console.log removed
    throw new Error(`Failed to discover URLs from ${inputUrl}: ${error}`);
  }
}

async function extractTextFromUrl(
  url: string,
  depth: number = 0,
): Promise<string> {
  // Prevent infinite redirect loops
  if (depth > 5) {
    // console.log removed
    throw new Error(`Too many redirects for ${url}`);
  }

  // Check if this is a slide page - force JavaScript rendering
  if (url.includes("/slide")) {
    // console.log removed
    try {
      const jsText = await extractTextUsingBrowser(url);
      // console.log removed
      if (jsText.length > 100) {
        return jsText;
      }
      // console.log removed
    } catch (jsError) {
      // console.log removed
    }
  }

  // Try regular extraction first
  try {
    // console.log removed

    // Create an AbortController for timeout
    const controller = new AbortController();
    const timeout = setTimeout(() => {
      controller.abort();
      // console.log removed
    }, 30000); // 30 second timeout

    const fetchStart = Date.now();
    const res = await fetch(url, {
      follow: 20, // Follow up to 20 HTTP redirects
      signal: controller.signal,
      headers: {
        "User-Agent":
          "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        Accept:
          "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate",
        Connection: "keep-alive",
      },
    });
    const fetchEnd = Date.now();

    clearTimeout(timeout);
    // console.log removed
    // console.log removed
    // console.log removed

    clearTimeout(timeout);

    if (!res.ok) {
      throw new Error(
        `Failed to fetch page: ${url} (Status: ${res.status} ${res.statusText})`,
      );
    }

    const html = await res.text();
    const $ = cheerio.load(html);

    // Check for HTML meta redirects
    const metaRefresh = $('meta[http-equiv="refresh"]').attr("content");
    if (metaRefresh) {
      const match = metaRefresh.match(/url=(.+)$/i);
      if (match) {
        let redirectUrl = match[1].trim();
        // console.log removed

        // Handle relative URLs by converting to absolute
        if (!redirectUrl.startsWith("http")) {
          try {
            const baseUrl = new URL(url);
            redirectUrl = new URL(redirectUrl, baseUrl.origin).href;
            // console.log removed
          } catch (urlError) {
            // console.log removed
            // If URL conversion fails, proceed with original content
          }
        }

        // Recursively fetch the redirect URL (with a simple depth limit)
        if (redirectUrl.startsWith("http")) {
          // console.log removed
          return extractTextFromUrl(redirectUrl, depth + 1);
        }
      }
    }

    // console.log removed
    $("script, style, noscript").remove();
    // Remove nav and footer elements, but be careful with headers
    $(
      "nav, footer, aside, .site-footer, .navbar, .global-nav, .cookie-banner, .newsletter, .modal, .offcanvas, .sidebar, .comments, .related-posts, .recommendations, .ad-container",
    ).remove();
    // Only remove header if it looks like a main site header (contains nav or is top level)
    $("body > header").remove();
    $("header:has(nav)").remove();

    const NOISE_PATTERNS = [
      /log\s*in/i,
      /sign\s*up/i,
      /get\s*a\s*demo/i,
      /talk\s*to\s*sales/i,
      /pricing/i,
      /help\s*center/i,
      /resource\s*center/i,
      /developer\s*tools/i,
      /become\s*a\s*partner/i,
      /careers/i,
    ];
    const isNoiseText = (t: string) => NOISE_PATTERNS.some((re) => re.test(t));
    const normalize = (t: string) => t.replace(/\s+/g, " ").trim();

    // Use body as scope to ensure we capture Hero sections that might be outside main/article
    // We rely on the robust removal logic above to strip out navs/footers/etc.
    const scope = $("body");

    const sections: string[] = [];
    let currentTitle = "";
    let currentContent: string[] = [];
    let sectionCount = 0;
    const seenBodies = new Set<string>();

    const pushSection = () => {
      const rawBody = normalize(currentContent.join(" "));
      const body = rawBody
        .split(" ")
        .filter((w) => w.length > 0)
        .join(" ");
      if (!body || body.length < 30) {
        // If the body is very short but we have a strong title, we might want to keep it?
        // But usually sections with < 30 chars are navigation noise or empty divs.
        currentTitle = "";
        currentContent = [];
        return;
      }
      // Only filter noise if text is very short (less than 100 chars)
      // This prevents dropping valid sections (like Heroes) that contain "Sign up" buttons
      if (body.length < 100 && isNoiseText(body)) {
        currentTitle = "";
        currentContent = [];
        return;
      }
      // Infer title if missing
      let title = normalize(currentTitle);
      if (!title) {
        title = body.split(".")[0].split(" ").slice(0, 8).join(" ");
      }
      // Suppress global promo banners commonly reused
      if (
        /the state of meetings 2024/i.test(title) &&
        !/report|state/i.test(url)
      ) {
        currentTitle = "";
        currentContent = [];
        return;
      }
      const key = (title + "::" + body.slice(0, 300)).toLowerCase();
      if (seenBodies.has(key)) {
        currentTitle = "";
        currentContent = [];
        return;
      }
      seenBodies.add(key);
      sectionCount += 1;
      sections.push(`[SECTION ${sectionCount}] ${title}\n${body}`);
      currentTitle = "";
      currentContent = [];
    };

    const contentSelector =
      "h1, h2, h3, h4, h5, h6, p, li, blockquote, td, th, div, article, section, dt, summary, legend, span, button, a";

    const blocklist = new Set([
      "script",
      "style",
      "noscript",
      "iframe",
      "svg",
      "img",
      "video",
      "audio",
      "canvas",
      "map",
      "object",
      "embed",
      "head",
      "meta",
      "link",
    ]);

    const traverse = ($node: any, isInsideSectionTag: boolean = false) => {
      $node.contents().each((_: number, el: any) => {
        if (el.type === "text") {
          const txt = normalize($(el).text());
          if (txt.length > 0) {
            currentContent.push(txt);
          }
        } else if (el.type === "tag") {
          const tagName = el.tagName.toLowerCase();
          if (blocklist.has(tagName)) return;

          const $el = $(el);

          // SPECIAL HANDLING FOR <SECTION> TAGS
          // The user requested that <section> tags should always be treated as a single unit,
          // preventing internal headers from splitting it into multiple crawler sections.
          if (tagName === "section") {
            // 1. Close any pending section from before this <section> tag
            if (currentTitle || currentContent.length) pushSection();

            // 2. Process this section recursively, with isInsideSectionTag = true
            // This flag will suppress header-based splitting inside this scope.
            traverse($el, true);

            // 3. Close this section immediately after finishing the tag
            if (currentTitle || currentContent.length) pushSection();
            return;
          }

          const role = $el.attr("role");
          const className = $el.attr("class") || "";
          const testId = $el.attr("data-testid") || "";

          // Check if this element acts as a container for other content elements
          const hasChildren = $el.find(contentSelector).length > 0;

          // Header detection: tags, roles, or class heuristics
          const isTagHeader =
            /^h[1-6]$/.test(tagName) ||
            ["dt", "summary", "legend"].includes(tagName) ||
            role === "heading";

          const isHero =
            /hero/i.test(testId) || /(\s|^)hero(\s|$)/i.test(className);

          const isClassHeader =
            /(\s|^)(section-title|section-header|headline|title|header|heading|h[1-6]|text-(xl|2xl|3xl|4xl|5xl))(\s|$)/i.test(
              className,
            ) || isHero;

          // A container is a header if it's explicitly a header tag,
          // OR if it looks like a header class AND doesn't contain structured content children.
          // (This allows <div class="hero">Title</div> to be a header,
          // but <section class="hero"><h1>Title</h1>...</section> to be a container)
          const isHeader = isTagHeader || (isClassHeader && !hasChildren);

          if (isInsideSectionTag) {
            // INSIDE A <SECTION>:
            // Do NOT split on headers. Instead, use the first header as the section title,
            // and treat subsequent headers as body text.
            if (isHeader) {
              const headerText = normalize($el.text());
              if (!currentTitle) {
                // First header becomes the title for this <section> block
                currentTitle = headerText;
              } else {
                // Subsequent headers are just appended to content
                if (headerText.length > 0) {
                  currentContent.push(headerText);
                }
              }
              // We consumed the header text, so do not recurse into its children
            } else {
              // Non-header elements: just recurse normally
              traverse($el, true);
            }
          } else {
            // NORMAL MODE (OUTSIDE <SECTION>):
            // Split on headers as usual.
            if (isHeader) {
              if (currentTitle || currentContent.length) pushSection();
              // Use .text() to get all text inside the header (e.g. <h1><span>Title</span></h1>)
              currentTitle = normalize($el.text());
              currentContent = [];
              // Do NOT recurse into header children (we consumed them as title)
            } else {
              traverse($el, false);
            }
          }
        }
      });
    };

    traverse(scope);
    if (currentTitle || currentContent.length) pushSection();

    const text =
      sections.length > 0
        ? sections.join("\n\n")
        : scope.text().replace(/\s+/g, " ").trim();

    // console.log removed
    // console.log removed

    // If the text is too short and this looks like a dynamic content page, try JavaScript extraction
    const contentPatterns = [
      /\/blog\//i,
      /\/post\//i,
      /\/article\//i,
      /\/slide\//i,
      /\/news\//i,
      /\/help\//i,
      /\/guide\//i,
      /\/tutorial\//i,
      /\/docs?\//i,
      /\/support\//i,
      /\/resource\//i,
      /\/case-stud/i,
      /\/faq\//i,
    ];

    const isContentPage = contentPatterns.some((pattern) => pattern.test(url));

    // For slide pages, be more aggressive about using JavaScript rendering
    const isSlidePageWithMinimalContent =
      url.includes("/slide") && text.length < 500;

    const hasNoSections = sections.length === 0;
    const hasFewSections = sections.length > 0 && sections.length < 3;

    // Trigger JS extraction if:
    // 1. Content is very short (< 500 chars) - likely loading state
    // 2. Slide page with minimal content
    // 3. No sections or few sections found - likely dynamic rendering masking structure
    // 4. Explicitly matches content patterns and is relatively short
    if (
      text.length < 500 ||
      isSlidePageWithMinimalContent ||
      hasNoSections ||
      hasFewSections ||
      (text.length < 1000 && isContentPage)
    ) {
      // console.log removed
      try {
        const jsText = await extractTextUsingBrowser(url);
        const jsSectionCount = (jsText.match(/\[SECTION\s+\d+\]/g) || [])
          .length;
        if (jsText.length > text.length || jsSectionCount > sections.length) {
          return jsText;
        }
      } catch (jsError) {
        // console.log removed
      }
    }

    // If the text is too short (likely a redirect page), log it
    if (text.length < 100) {
      // console.log removed
    }

    return text;
  } catch (error) {
    // console.log removed
    // If regular extraction fails completely, try JavaScript as fallback
    try {
      return await extractTextUsingBrowser(url);
    } catch (jsError) {
      // console.error removed
      throw error; // Throw the original error
    }
  }
}

async function extractTextUsingBrowser(url: string): Promise<string> {
  // console.log removed

  let browser;
  try {
    browser = await puppeteer.launch({
      headless: true,
      args: [
        "--no-sandbox",
        "--disable-setuid-sandbox",
        "--disable-dev-shm-usage",
        "--disable-accelerated-2d-canvas",
        "--no-first-run",
        "--no-zygote",
        "--disable-gpu",
      ],
    });

    const page = await browser.newPage();
    // Enable console logging from browser context
    page.on("console", (msg) => {
      console.log(`[BROWSER LOG]: ${msg.text()}`);
    });
    await page.setViewport({ width: 1280, height: 720 });
    await page.setUserAgent(
      "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    );

    // Navigate and wait for content to load
    await page.goto(url, {
      waitUntil: "networkidle2",
      timeout: 30000,
    });

    // Wait for dynamic content - longer wait for slides
    const waitTime = url.includes("/slide") ? 5000 : 3000;
    // console.log removed
    await new Promise((resolve) => setTimeout(resolve, waitTime));

    // Scroll to trigger lazy loading/animations for all pages
    // console.log removed
    await page.evaluate(async () => {
      await new Promise<void>((resolve) => {
        let totalHeight = 0;
        const distance = 400; // Scroll in larger chunks
        const timer = setInterval(() => {
          const scrollHeight = document.body.scrollHeight;
          window.scrollBy(0, distance);
          totalHeight += distance;

          if (totalHeight >= scrollHeight) {
            clearInterval(timer);
            resolve();
          }
        }, 100);
      });
    });
    // Wait a bit after scrolling for final animations
    await new Promise((resolve) => setTimeout(resolve, 2000));

    // Extract text content from the rendered page
    const text = await page.evaluate(() => {
      // Remove script and noise elements (BUT KEEP STYLES for computed style checks)
      const scripts = document.querySelectorAll(
        "script, noscript, nav, footer, aside, .site-header, .site-footer, .navbar, .global-nav, .global-header, .cookie-banner, .newsletter, .modal, .offcanvas",
      );
      scripts.forEach((el) => el.remove());

      // Remove main header if it contains nav
      const mainHeader = document.querySelector("body > header");
      if (
        mainHeader &&
        (mainHeader.querySelector("nav") || mainHeader.tagName === "HEADER")
      ) {
        mainHeader.remove();
      }

      // Define content selectors
      const contentSelector =
        "h1, h2, h3, h4, h5, h6, p, li, blockquote, td, th, div, article, section, dt, summary, legend, span, button, a";

      // Helper to check if an element is a header
      const isHeaderElement = (el: Element) => {
        const tagName = el.tagName.toLowerCase();
        const role = el.getAttribute("role");
        const className = typeof el.className === "string" ? el.className : "";
        const text = el.textContent || "";

        // Skip if text is too long (headers are usually short)
        if (text.length > 200) return false;

        // Tag based
        if (
          /^h[1-6]$/.test(tagName) ||
          ["dt", "summary", "legend"].includes(tagName)
        )
          return true;

        // Role based
        if (role === "heading") return true;

        // Class based (only if leaf node or minimal children)
        if (
          /(\s|^)(section-title|section-header|headline|title|header|heading|h[1-6]|text-(xl|2xl|3xl|4xl|5xl))(\s|$)/i.test(
            className,
          )
        ) {
          // Check if it's not a container for other content
          return el.children.length === 0 || text.length < 100;
        }

        // Computed Style based (Visual Headers)
        try {
          const style = window.getComputedStyle(el);
          const fontSize = parseFloat(style.fontSize);
          const fontWeight = style.fontWeight;
          const fontWeightVal = parseInt(fontWeight);
          const isBold =
            fontWeight === "bold" ||
            fontWeight === "bolder" ||
            (!isNaN(fontWeightVal) && fontWeightVal >= 600);

          // Heuristic: Large font OR (Bold + slightly larger than base)
          // Assuming base is 16px.
          if (fontSize >= 24) return true; // Very large text is likely a header
          if (fontSize >= 18 && isBold) return true; // Large bold text

          // Special case for strong/b tags that are direct children of body/main (unlikely but possible)
          if ((tagName === "strong" || tagName === "b") && fontSize >= 16)
            return true;
        } catch (e) {}

        return false;
      };

      const sections: string[] = [];
      let headerCount = 0;
      let currentTitle = "";
      let currentContent: string[] = [];
      let sectionCount = 0;
      const seenBodies = new Set<string>();

      const normalize = (t: string) => t.replace(/\s+/g, " ").trim();

      const pushSection = () => {
        const rawBody = normalize(currentContent.join(" "));
        if (!rawBody || rawBody.length < 20) {
          // If we have a title but no body, keep the title?
          // No, usually meaningless.
          // But maybe the title IS the content (e.g. strict Q&A).
          // Reset for now.
          if (!currentTitle) {
            currentTitle = "";
            currentContent = [];
            return;
          }
          // If we have a title, we might want to keep it if it's substantial?
          // For now, stick to the limit.
        }

        // Deduplication
        const key = (currentTitle + "::" + rawBody.slice(0, 300)).toLowerCase();
        if (seenBodies.has(key)) {
          currentTitle = "";
          currentContent = [];
          return;
        }
        seenBodies.add(key);

        // Infer title if missing
        let title = normalize(currentTitle);
        if (!title && rawBody) {
          title = rawBody.split(".")[0].split(" ").slice(0, 8).join(" ");
        }

        if (title || rawBody) {
          sectionCount++;
          sections.push(`[SECTION ${sectionCount}] ${title}\n${rawBody}`);
        }
        currentTitle = "";
        currentContent = [];
      };

      // Traverse the DOM recursively to extract sections
      const traverse = (node: Node) => {
        // Handle Text Nodes
        if (node.nodeType === Node.TEXT_NODE) {
          const text = node.textContent?.trim();
          if (text && text.length > 0) {
            currentContent.push(text);
          }
          return;
        }

        // Handle Elements
        if (node.nodeType === Node.ELEMENT_NODE) {
          const el = node as Element;

          // Skip noise (should have been removed, but double check if needed)
          // (Scripts etc were removed earlier)

          // Check if Header
          if (isHeaderElement(el)) {
            if (currentTitle || currentContent.length > 0) pushSection();

            headerCount++;
            console.log(
              `[DEBUG] Found header: ${el.tagName} class=${el.className} text=${el.textContent?.substring(0, 30)}`,
            );

            currentTitle = el.textContent || "";
            currentContent = [];

            // Do NOT traverse children of a header (we took its text)
            return;
          }

          // Not a header, traverse children
          node.childNodes.forEach((child) => traverse(child));
        }
      };

      const main =
        document.querySelector("main") ||
        document.querySelector("[role='main']") ||
        document.body;

      traverse(main);

      if (currentTitle || currentContent.length > 0) pushSection();

      console.log(
        `[DEBUG] extractTextUsingBrowser: Found ${headerCount} headers and generated ${sections.length} sections`,
      );

      return sections.length > 0
        ? sections.join("\n\n")
        : document.body.innerText.replace(/\s+/g, " ").trim();
    });

    // console.log removed

    return text;
  } catch (error) {
    // console.log removed
    throw error;
  } finally {
    if (browser) {
      await browser.close();
    }
  }
}

export async function POST(req: NextRequest) {
  // Clone the request to read body without consuming it for processBatch
  const clone = req.clone();
  let body;
  try {
    body = await clone.json();
  } catch (e) {
    return processBatch(req);
  }

  // Get Admin ID (needed for stop action and background loop)
  let adminId: string | null = null;
  const cookieAuth = verifyAdminTokenFromCookie(req);
  if (cookieAuth) {
    adminId = cookieAuth.adminId;
  } else {
    const headerAuth = verifyAdminToken(req);
    if (headerAuth) {
      adminId = headerAuth.adminId;
    }
  }
  // Fallback to API key if needed
  if (!adminId) {
    const apiKey =
      req.headers.get("x-api-key") ||
      req.headers.get("X-API-Key") ||
      req.headers.get("api-key") ||
      req.headers.get("Api-Key");
    if (apiKey) {
      const apiAuth = await verifyApiKey(apiKey);
      if (apiAuth) adminId = apiAuth.adminId;
    }
  }

  // Handle Stop Action
  if (body.action === "stop") {
    if (!adminId) {
      return NextResponse.json({ error: "Unauthorized" }, { status: 401 });
    }
    const db = await getDb();
    const crawlStates = db.collection("crawl_states");
    await crawlStates.updateOne(
      { adminId },
      { $set: { status: "stopped", updatedAt: new Date() } },
      { upsert: true },
    );
    // console.log removed
    return NextResponse.json({ message: "Stop signal received" });
  }

  if (!body.background) {
    // If not background, this is the FIRST request.
    // We MUST set status to 'running' here synchronously before starting the process
    // This ensures the UI sees 'running' immediately.

    // Extract Admin ID for status update
    let currentAdminId: string | null = null;
    if (adminId) {
      currentAdminId = adminId;
    } else {
      // Try to get admin ID from token/key if not already resolved
      const cookieToken = req.cookies.get("auth_token")?.value;
      if (cookieToken) {
        try {
          const payload = jwt.verify(cookieToken, JWT_SECRET) as {
            adminId: string;
          };
          currentAdminId = payload.adminId;
        } catch {}
      }
    }

    if (currentAdminId) {
      const db = await getDb();
      await db
        .collection("crawl_states")
        .updateOne(
          { adminId: currentAdminId },
          { $set: { status: "running", updatedAt: new Date() } },
          { upsert: true },
        );
    }

    return processBatch(req);
  }

  const cookieToken = req.cookies.get("auth_token")?.value;
  let token = cookieToken;
  if (!token) {
    const authHeader =
      req.headers.get("authorization") || req.headers.get("Authorization");
    if (authHeader) {
      const match = authHeader.match(/Bearer\s+(.+)/i);
      token = match ? match[1] : authHeader;
    }
  }

  const sitemapUrl = body.sitemapUrl;
  if (!sitemapUrl) {
    return NextResponse.json({ error: "Missing sitemapUrl" }, { status: 400 });
  }

  const internalUrl = new URL("/api/sitemap", req.nextUrl.origin).toString();

  // Start background crawl process (Fire and Forget)
  (async () => {
    try {
      // console.log removed

      // Set status to running
      if (adminId) {
        const db = await getDb();
        await db
          .collection("crawl_states")
          .updateOne(
            { adminId },
            { $set: { status: "running", updatedAt: new Date() } },
            { upsert: true },
          );
      }

      let hasMore = true;
      let batchCount = 0;
      const MAX_BATCHES = 50; // Safety limit: ~300 pages

      while (hasMore && batchCount < MAX_BATCHES) {
        // Check for stop signal
        if (adminId) {
          const db = await getDb();
          const state = await db
            .collection("crawl_states")
            .findOne({ adminId });
          if (state?.status === "stopped") {
            // console.log removed
            break;
          }
        }

        // console.log removed

        const headers: Record<string, string> = {
          "Content-Type": "application/json",
        };
        if (token) {
          headers["Cookie"] = `auth_token=${token}`;
          headers["Authorization"] = `Bearer ${token}`;
        }

        const resp = await fetch(internalUrl, {
          method: "POST",
          headers,
          body: JSON.stringify({ sitemapUrl, background: false }),
        });

        if (!resp.ok) {
          // console.error removed
          break;
        }

        let data: any = null;
        try {
          data = await resp.json();
        } catch (e) {
          // console.error removed
          break;
        }

        hasMore = data?.hasMorePages || false;
        batchCount++;

        if (hasMore) {
          // Small delay to prevent overwhelming the server
          await new Promise((resolve) => setTimeout(resolve, 2000));
        }
      }

      // Mark as completed when done (if not stopped)
      if (adminId) {
        const db = await getDb();
        const finalState = await db
          .collection("crawl_states")
          .findOne({ adminId });
        if (finalState?.status !== "stopped") {
          await db
            .collection("crawl_states")
            .updateOne(
              { adminId },
              { $set: { status: "completed", updatedAt: new Date() } },
            );
        }
      }

      // console.log removed
    } catch (err) {
      // console.error removed
    }
  })();

  return NextResponse.json(
    {
      message: "Crawl started in background",
      status: "started",
      details:
        "Pages are being crawled in the background. They will appear in the dashboard as they are processed.",
    },
    { status: 200 },
  );
}

async function processBatch(req: NextRequest) {
  const startTime = Date.now();
  const MAX_EXECUTION_TIME = 240000; // 240 seconds (4 minutes) to stay within Vercel 300s limit

  // console.log removed
  // console.log removed

  let adminId: string | null = null;

  const cookieToken = req.cookies.get("auth_token")?.value;
  let token = cookieToken;
  if (!token) {
    // console.log removed
    const authHeader =
      req.headers.get("authorization") || req.headers.get("Authorization");
    if (authHeader) {
      const match = authHeader.match(/Bearer\s+(.+)/i);
      token = match ? match[1] : authHeader;
    }
  }

  if (token) {
    // console.log removed
    try {
      const payload = jwt.verify(token, JWT_SECRET) as {
        email: string;
        adminId: string;
      };
      adminId = payload.adminId;
      // console.log removed
    } catch (authError) {
      // console.log removed
    }
  } else {
    // console.log removed
  }

  if (!adminId) {
    const apiKey =
      req.headers.get("x-api-key") ||
      req.headers.get("X-API-Key") ||
      req.headers.get("api-key") ||
      req.headers.get("Api-Key");
    if (apiKey) {
      // console.log removed
      const apiAuth = await verifyApiKey(apiKey);
      if (apiAuth) {
        adminId = apiAuth.adminId;
        // console.log removed
      } else {
        // console.log removed
      }
    }
  }

  if (!adminId) {
    // console.log removed
    return NextResponse.json({ error: "Unauthorized" }, { status: 401 });
  }

  // console.log removed
  const body = await req.json();
  // console.log removed

  const { sitemapUrl, retryUrl } = body;

  // Helper for URL normalization
  const normalizeUrl = (u: string) => {
    try {
      const urlObj = new URL(u);
      return urlObj.origin + urlObj.pathname.replace(/\/$/, "");
    } catch (e) {
      return u.replace(/\/$/, "");
    }
  };

  if (retryUrl) {
    const normalizedRetryUrl = normalizeUrl(retryUrl);
    // console.log removed
    const db = await getDb();
    const sitemapUrls = db.collection("sitemap_urls");
    const pages = db.collection("crawled_pages");
    const pineconeVectors = db.collection("pinecone_vectors");
    const structuredSummaries = db.collection("structured_summaries");

    const enrichStructuredSummaryWithQuestions = async (
      summary: any,
      pageText: string,
    ) => {
      if (!summary || !pageText || typeof summary !== "object") return summary;
      const sections = Array.isArray(summary.sections) ? summary.sections : [];
      if (!sections.length) return summary;
      let blocks = parseSectionBlocks(pageText);
      // Use a lower threshold (30 chars) to preserve valid small sections
      const minChars = 30;
      blocks =
        Array.isArray(blocks) && blocks.length > 0
          ? mergeSmallSectionBlocks(blocks, minChars)
          : blocks;
      console.log(
        `[enrichStructuredSummaryWithQuestions] Processing ${sections.length} sections with ${blocks.length} blocks for retryUrl`,
      );
      // Process sections sequentially to avoid rate limits and ensure reliability
      const enrichedSections = [];
      for (let idx = 0; idx < sections.length; idx++) {
        const sec = sections[idx];
        try {
          const name = String(sec?.sectionName || `Section ${idx + 1}`);
          const sectionSummary = String(sec?.sectionSummary || "");

          // Improved block matching
          let block = blocks.find(
            (b) =>
              b.title && name && b.title.toLowerCase() === name.toLowerCase(),
          );

          // Fallback matching: try partial match or index
          if (!block) {
            block = blocks[idx];
          }

          // Final fallback
          if (!block) {
            block = { title: name, body: sectionSummary };
          }

          const sectionType = classifySectionType(
            name,
            sectionSummary,
            String(block.body || ""),
          );
          console.log(
            `[enrichStructuredSummaryWithQuestions] Generating questions for section ${
              idx + 1
            }/${sections.length}: "${name}"`,
          );

          // Add a small delay between requests to be nice to the API
          if (idx > 0) await new Promise((resolve) => setTimeout(resolve, 500));

          const questionsData = await refineSectionQuestions(
            openai,
            retryUrl,
            String(summary?.pageType || "other"),
            String(idx + 1),
            name,
            String(block.body || ""),
            sectionSummary,
            sectionType as any,
          );

          if (!questionsData) {
            console.warn(
              `[enrichStructuredSummaryWithQuestions] No questions generated for section ${
                idx + 1
              }`,
            );
          }

          if (questionsData) {
            sec.leadQuestions = (
              Array.isArray(questionsData.leadQuestions)
                ? questionsData.leadQuestions
                : []
            )
              .slice(0, 2)
              .map((lq: any) => {
                let opts = Array.isArray(lq?.options)
                  ? lq.options.map((o: any) => ({
                      label: String(o?.label || o || ""),
                      tags: Array.isArray(o?.tags)
                        ? o.tags.map((t: any) => snakeTag(String(t)))
                        : [],
                      workflow:
                        typeof o?.workflow === "string"
                          ? o.workflow
                          : "ask_sales_question",
                    }))
                  : [];
                if (opts.length < 2) {
                  while (opts.length < 2)
                    opts.push({
                      label: `Option ${opts.length + 1}`,
                      tags: [],
                      workflow: "ask_sales_question",
                    });
                }
                if (opts.length > 4) opts = opts.slice(0, 4);
                return {
                  question: String(lq?.question || ""),
                  options: opts,
                  tags: [],
                  workflow: "ask_sales_question",
                };
              });

            sec.salesQuestions = (
              Array.isArray(questionsData.salesQuestions)
                ? questionsData.salesQuestions
                : []
            )
              .slice(0, 2)
              .map((sq: any) => {
                let opts = Array.isArray(sq?.options)
                  ? sq.options.map((o: any) => ({
                      label: String(o?.label || o || ""),
                      tags: Array.isArray(o?.tags)
                        ? o.tags.map((t: any) => snakeTag(String(t)))
                        : [],
                      workflow:
                        typeof o?.workflow === "string"
                          ? o.workflow
                          : "diagnostic_education",
                    }))
                  : [];
                if (opts.length < 2) {
                  while (opts.length < 2)
                    opts.push({
                      label: `Option ${opts.length + 1}`,
                      tags: [],
                      workflow: "diagnostic_education",
                    });
                }
                if (opts.length > 4) opts = opts.slice(0, 4);

                const ensuredFlows = opts.map((o: any) => ({
                  forOption: o.label,
                  diagnosticAnswer: "",
                  followUpQuestion: "",
                  followUpOptions: [],
                  featureMappingAnswer: "",
                  loopClosure: "",
                }));

                return {
                  question: String(sq?.question || ""),
                  options: opts,
                  tags: [],
                  workflow: "diagnostic_response",
                  optionFlows: ensuredFlows,
                };
              });

            // Apply standalone tag generation
            try {
              sec.leadQuestions = await processQuestionsWithTags(
                sec.leadQuestions,
                block.body,
                adminId || undefined,
                summary?.businessName,
              );
              sec.salesQuestions = await processQuestionsWithTags(
                sec.salesQuestions,
                block.body,
                adminId || undefined,
                summary?.businessName,
              );
            } catch (tagError) {
              console.error(
                `[Retry] Tag generation failed for section ${idx}:`,
                tagError,
              );
            }
          }
        } catch (err) {
          console.error(
            `[enrichStructuredSummaryWithQuestions] Error processing section ${
              idx + 1
            }:`,
            err,
          );
        }
        enrichedSections.push(sec);
      }
      summary.sections = enrichedSections;
      return normalizeStructuredSummary(summary);
    };

    // Find the failed entry to get context (sitemapUrl)
    const failedEntry = await sitemapUrls.findOne({ adminId, url: retryUrl });
    if (!failedEntry) {
      return NextResponse.json(
        { error: "URL not found in history" },
        { status: 404 },
      );
    }

    const sitemapUrlContext = failedEntry.sitemapUrl;

    // Reset status for ALL matching URLs (handles duplicates across sitemaps)
    // await sitemapUrls.updateMany(
    //   { adminId, url: retryUrl },
    //   {
    //     $unset: { failedAt: 1, error: 1 },
    //     $set: { recrawlAt: new Date(), recrawlReason: "user_retry" },
    //   },
    // );

    try {
      const retryUrlTrimmed = retryUrl.trim();
      const retryUrlNoSlash = retryUrlTrimmed.replace(/\/+$/, "");
      const retryUrlWithSlash =
        retryUrlNoSlash.length > 0 ? `${retryUrlNoSlash}/` : retryUrlTrimmed;

      const urlVariants =
        retryUrlNoSlash === retryUrlWithSlash
          ? [retryUrlNoSlash]
          : [retryUrlNoSlash, retryUrlWithSlash];

      const urlConditions = urlVariants.map((u) => ({
        url: {
          $regex: new RegExp(
            `^${u.replace(/[.*+?^${}()|[\]\\]/g, "\\$&")}$`,
            "i",
          ),
        },
      }));
      const filenameConditions = urlVariants.map((u) => ({
        filename: {
          $regex: new RegExp(
            `^${u.replace(/[.*+?^${}()|[\]\\]/g, "\\$&")}$`,
            "i",
          ),
        },
      }));

      await pages.deleteMany({ adminId, $or: urlConditions });
      await structuredSummaries.deleteMany({
        adminId,
        $or: urlConditions,
      });
      await pineconeVectors.deleteMany({
        adminId,
        $or: filenameConditions,
      });
      // Delete duplicates from sitemapUrls, keeping only the current failed entry (by ID)
      await sitemapUrls.deleteMany({
        adminId,
        $or: urlConditions,
        _id: { $ne: failedEntry._id },
      });

      for (const u of urlVariants) {
        await deleteChunksByUrl(u, adminId);
      }

      // console.log removed
      let text = await extractTextFromUrl(retryUrl);
      const rawBlocks = parseSectionBlocks(text);
      // Use a lower threshold (30 chars) to preserve valid small sections
      const minChars = 30;
      const mergedBlocks =
        rawBlocks.length > 0
          ? mergeSmallSectionBlocks(rawBlocks, minChars)
          : rawBlocks;
      if (mergedBlocks.length > 0) {
        text = blocksToSectionedText(mergedBlocks);
      }

      // Generate basic summary
      const basicSummaryResponse = await openai.chat.completions.create({
        model: "gpt-4o-mini",
        messages: [
          {
            role: "system",
            content:
              "You are a helpful assistant that creates concise summaries of web page content. Focus on the main topics, key information, and important details.",
          },
          {
            role: "user",
            content: `Please create a concise summary of the following web page content:\n\n${text}`,
          },
        ],
        max_tokens: 300,
        temperature: 0.3,
      });
      const basicSummary =
        basicSummaryResponse.choices[0]?.message?.content ||
        "Summary not available";

      // Generate structured summary
      let structuredSummary: any = null;
      if (text && text.trim().length > 0) {
        try {
          try {
            structuredSummary = await generateStructuredSummaryFromText(
              text || "",
            );
            if (structuredSummary) {
              structuredSummary = normalizeStructuredSummary(structuredSummary);
            }
          } catch (e) {
            // console.error removed
          }

          if (!structuredSummary && text && text.trim().length > 0) {
            const fallback = buildFallbackStructuredSummaryFromText(text);
            if (fallback) {
              structuredSummary = normalizeStructuredSummary(fallback);
              structuredSummary = await enrichStructuredSummaryWithQuestions(
                structuredSummary,
                text,
              );
              console.log(
                `[Retry] Used fallback structured summary for ${retryUrl}`,
              );
            }
          }
        } catch (summaryError) {
          // console.error removed
        }
      }

      const pageData: any = {
        adminId,
        url: normalizedRetryUrl,
        text,
        summary: basicSummary,
        filename: normalizedRetryUrl,
        createdAt: new Date(),
      };

      if (structuredSummary) {
        pageData.summaryGeneratedAt = new Date();
      }

      // Use updateOne with upsert instead of insertOne to prevent duplicates
      const updateResult = await pages.updateOne(
        { adminId, url: normalizedRetryUrl },
        { $set: pageData },
        { upsert: true },
      );

      let pageId;
      if (updateResult.upsertedId) {
        pageId = updateResult.upsertedId;
      } else {
        const existingPage = await pages.findOne({
          adminId,
          url: normalizedRetryUrl,
        });
        pageId = existingPage?._id;
      }

      if (structuredSummary && pageId) {
        await structuredSummaries.updateOne(
          { adminId, pageId },
          {
            $set: {
              adminId,
              pageId,
              url: normalizedRetryUrl,
              structuredSummary,
              summaryGeneratedAt: new Date(),
            },
          },
          { upsert: true },
        );
      }

      // Mark as crawled for ALL matching URLs (which should be just one now)
      // And clear error state
      await sitemapUrls.updateMany(
        { adminId, url: normalizedRetryUrl },
        {
          $set: { crawled: true, crawledAt: new Date() },
          $unset: { failedAt: 1, error: 1 },
        },
      );

      // Chunk and embed
      let chunks = chunkText(text);
      if (chunks.length === 0 && text.length > 10) {
        chunks = [text.trim()];
      }

      if (chunks.length > 0) {
        // console.log removed
        try {
          const embeddings = await Promise.all(
            chunks.map(async (chunk) => {
              const response = await openai.embeddings.create({
                model: "text-embedding-3-small",
                input: chunk,
                dimensions: 1024,
              });
              return response.data[0].embedding;
            }),
          );

          const vectors = chunks.map((chunk, i) => ({
            id: `${retryUrl}-${i}`,
            values: embeddings[i],
            metadata: {
              url: retryUrl,
              text: chunk,
              chunkIndex: i,
              adminId,
              sitemapUrl: sitemapUrlContext,
              createdAt: new Date().toISOString(),
            },
          }));

          await index.upsert(vectors);

          // Backup vectors to MongoDB
          await pineconeVectors.deleteMany({ adminId, filename: retryUrl });
          const vectorDocs = vectors.map((v) => ({
            adminId,
            vectorId: v.id,
            filename: retryUrl,
            sitemapUrl: sitemapUrlContext,
            createdAt: new Date(),
          }));
          if (vectorDocs.length > 0) {
            await pineconeVectors.insertMany(vectorDocs);
          }
        } catch (embedError) {
          // console.error removed
        }
      }

      return NextResponse.json({
        message: "Retry successful",
        url: retryUrl,
        crawled: true,
      });
    } catch (err) {
      // console.error removed
      // Mark as failed
      await sitemapUrls.updateOne(
        { _id: failedEntry._id },
        {
          $set: {
            failedAt: new Date(),
            error: err instanceof Error ? err.message : String(err),
          },
        },
      );
      return NextResponse.json(
        { error: err instanceof Error ? err.message : String(err) },
        { status: 500 },
      );
    }
  }

  if (!sitemapUrl) {
    // console.log removed
    return NextResponse.json({ error: "Missing URL" }, { status: 400 });
  }

  // console.log removed
  try {
    // Normalize URL to ensure HTTPS and proper format
    let normalizedUrl = sitemapUrl.trim();

    // Add protocol if missing
    if (
      !normalizedUrl.startsWith("http://") &&
      !normalizedUrl.startsWith("https://")
    ) {
      normalizedUrl = "https://" + normalizedUrl;
    }

    // Convert HTTP to HTTPS for better compatibility
    if (normalizedUrl.startsWith("http://")) {
      normalizedUrl = normalizedUrl.replace("http://", "https://");
    }

    // console.log removed
    // console.log removed

    let urls: string[] = [];
    let discoveryType: "sitemap" | "webpage" | "javascript" = "sitemap";
    try {
      // console.log removed
      const result = await discoverUrls(normalizedUrl, adminId);
      urls = result.urls;
      discoveryType = result.type;
      // Clean and filter discovered URLs to the requested locale (reduces batch size)
      const cleanedUrls = urls.map((u) => u.trim().replace(/,$/, ""));
      if (discoveryType === "sitemap") {
        try {
          const u = new URL(normalizedUrl);
          const localeMatch = u.pathname.match(/\/hc\/(\w[\w-]*)/);
          const locale = localeMatch ? localeMatch[1] : null;
          const originHost = u.hostname;

          // FIX: Rewrite URLs to match the requested domain
          // This fixes the issue where a staging site (vercel.app) has a sitemap
          // generated for production (custom domain), causing all links to be filtered out.
          const rewrittenUrls = cleanedUrls
            .map((link) => {
              try {
                const linkUrl = new URL(link);
                if (!isSameDomain(linkUrl.hostname, originHost)) {
                  // Keep the path and query, but switch to the requested host
                  linkUrl.protocol = u.protocol;
                  linkUrl.hostname = originHost;
                  linkUrl.port = u.port;
                  return linkUrl.href;
                }
                return link;
              } catch {
                return null;
              }
            })
            .filter((l): l is string => l !== null);

          const filtered = rewrittenUrls.filter((link) => {
            try {
              const lu = new URL(link);
              const sameHost = isSameDomain(lu.hostname, originHost);
              const sameLocale =
                !locale || lu.pathname.includes(`/hc/${locale}`);
              return sameHost && sameLocale;
            } catch {
              return false;
            }
          });
          urls = Array.from(new Set(filtered));
        } catch {
          urls = Array.from(new Set(cleanedUrls));
        }
      } else {
        urls = Array.from(new Set(cleanedUrls));
      }
      // console.log removed
      // console.log removed
    } catch (error) {
      // console.error removed
      return NextResponse.json(
        { error: `Failed to discover URLs from the provided link: ${error}` },
        { status: 400 },
      );
    }

    // console.log removed
    const db = await getDb();

    // Check if stopped BEFORE overwriting
    // This prevents the race condition where user clicks stop, but this function overwrites it to running
    const crawlStates = db.collection("crawl_states");
    const currentCrawlState = await crawlStates.findOne({ adminId });
    if (currentCrawlState?.status === "stopped") {
      return NextResponse.json({
        crawled: 0,
        totalChunks: 0,
        pages: [],
        failedPages: [],
        batchDone: 0,
        batchRemaining: 0,
        totalRemaining: 0,
        recrawledPages: 0,
        timeoutReached: false,
        executionTime: 0,
        totalDiscovered: 0,
        hasMorePages: false,
        sitemapUrl,
        message: "Crawl stopped by user",
      });
    }

    // Reset crawl state to ensure we don't hit a stale stop signal
    await crawlStates.updateOne(
      { adminId },
      { $set: { status: "running", updatedAt: new Date() } },
      { upsert: true },
    );

    const pages = db.collection("crawled_pages");
    const sitemapUrls = db.collection("sitemap_urls");
    const adminSettings = await getAdminSettingsCollection();

    // Store the last submitted sitemapUrl for this admin
    await adminSettings.updateOne(
      { adminId },
      { $set: { lastSitemapUrl: sitemapUrl } },
      { upsert: true },
    );

    // Store all sitemap URLs for this admin with the specific sitemapUrl context
    const now = new Date();

    // Ensure no duplicate URLs before creating docs
    const uniqueUrls = Array.from(new Set(urls.map((u) => normalizeUrl(u))));
    if (uniqueUrls.length !== urls.length) {
      console.log(
        `[processBatch] Deduped URLs from ${urls.length} to ${uniqueUrls.length}`,
      );
    }

    const sitemapUrlDocs = uniqueUrls.map((url) => ({
      adminId,
      url,
      sitemapUrl, // This ensures each sitemap submission is tracked separately
      addedAt: now,
      crawled: false,
      discoveryType, // Track how this URL was discovered
    }));
    if (sitemapUrlDocs.length > 0) {
      const ops = sitemapUrlDocs.map((doc) => ({
        updateOne: {
          filter: {
            adminId: doc.adminId,
            url: doc.url,
            sitemapUrl: doc.sitemapUrl,
          },
          update: { $setOnInsert: doc },
          upsert: true,
        },
      }));
      const CHUNK = 500;
      for (let i = 0; i < ops.length; i += CHUNK) {
        const chunk = ops.slice(i, i + CHUNK);
        await sitemapUrls.bulkWrite(chunk, { ordered: false });
      }
    }

    // Find already crawled URLs for this specific admin/sitemapUrl combination
    const crawledDocs = await sitemapUrls
      .find({ adminId, sitemapUrl, crawled: true }) // This now only looks at URLs from this specific sitemap submission
      .toArray();

    // Also check for pages that were marked as crawled but have no chunks in Pinecone
    // This can happen if they were redirect pages or had errors during processing
    const pineconeVectors = db.collection("pinecone_vectors");
    const problematicUrls: string[] = [];

    // console.log removed
    for (const doc of crawledDocs) {
      // Check if vectors exist in Pinecone by trying to fetch them
      const vectorIds = await pineconeVectors
        .find({ adminId, filename: doc.url })
        .project({ vectorId: 1, _id: 0 })
        .toArray();

      if (vectorIds.length === 0) {
        // console.log removed
        problematicUrls.push(doc.url);
      } else {
        // Check if the vectors actually exist in Pinecone
        try {
          const vectorIdList = vectorIds.map(
            (v) => (v as { vectorId: string }).vectorId,
          );
          const result = await index.fetch(vectorIdList);
          const foundVectors = Object.keys(result.records || {}).length;
          // console.log removed

          if (foundVectors === 0) {
            // console.log removed
            problematicUrls.push(doc.url);
          }
        } catch (pineconeError) {
          // console.log removed
          problematicUrls.push(doc.url);
        }
      }

      if (problematicUrls.includes(doc.url)) {
        // Reset the crawled status so it can be re-crawled
        await sitemapUrls.updateOne(
          { adminId, url: doc.url, sitemapUrl }, // Include sitemapUrl in the query
          {
            $unset: { crawled: 1, crawledAt: 1 },
            $set: {
              recrawlReason: "no_pinecone_vectors",
              recrawlAt: new Date(),
            },
          },
        );
      }
    }

    const results: { url: string; text: string }[] = [];
    const failedUrls: { url: string; error: string }[] = [];
    let totalChunks = 0;
    let crawlCount = 0;
    let timeoutReached = false;
    let updatedCrawledUrls = new Set<string>();
    const processedInSession = new Set<string>();

    while (true) {
      if (Date.now() - startTime > MAX_EXECUTION_TIME) {
        // console.log removed
        timeoutReached = true;
        break;
      }

      // 1. Fetch pages already crawled FOR THIS SITEMAP
      const updatedCrawledDocs = await sitemapUrls
        .find({ adminId, crawled: true })
        .project({ url: 1, _id: 0 })
        .toArray();
      const sitemapCrawledSet = new Set(
        updatedCrawledDocs.map((doc: any) => doc.url),
      );

      // 2. Fetch pages crawled GLOBALLY for this admin (to avoid re-crawling content)
      const existingPagesDocs = await pages
        .find({ adminId })
        .project({ url: 1, _id: 0 })
        .toArray();
      const globalCrawledSet = new Set(
        existingPagesDocs.map((doc: any) => doc.url),
      );

      // 3. Remove problematic URLs from global set to force re-crawl
      problematicUrls.forEach((url) => globalCrawledSet.delete(url));

      // 4. Identify URLs that need processing (either crawl or status update)
      // We do NOT filter out globalCrawledSet here, because we want to iterate over them
      // and update their status in the sitemapUrls collection without re-fetching.
      // FIX: Also check if globalCrawledSet is missing the URL (meaning content was deleted),
      // even if sitemapUrls says it was crawled. This allows self-correction.
      const uncrawledUrls = urls
        .filter(
          (url) =>
            (!sitemapCrawledSet.has(url) || !globalCrawledSet.has(url)) &&
            !processedInSession.has(url),
        )
        .slice(0, MAX_PAGES);

      if (uncrawledUrls.length === 0) {
        // console.log removed
        break;
      }

      // console.log removed
      // console.log removed
      // console.log removed

      for (const url of uncrawledUrls) {
        // Fast-Track: If already crawled globally, skip fetch but update sitemap status
        // Check for URL variations (trailing slash, protocol) to ensure we don't re-crawl
        // just because of minor format differences

        const variations = [url];

        // Add trailing slash variation
        if (url.endsWith("/")) {
          variations.push(url.slice(0, -1));
        } else {
          variations.push(`${url}/`);
        }

        // Add protocol variation (http <-> https)
        const isHttps = url.startsWith("https://");
        const altProtocol = isHttps
          ? url.replace("https://", "http://")
          : url.replace("http://", "https://");
        variations.push(altProtocol);

        // Add combined variation (protocol + slash)
        if (altProtocol.endsWith("/")) {
          variations.push(altProtocol.slice(0, -1));
        } else {
          variations.push(`${altProtocol}/`);
        }

        const isAlreadyCrawled = variations.some((v) =>
          globalCrawledSet.has(v),
        );

        if (isAlreadyCrawled) {
          // console.log removed
          await sitemapUrls.updateOne(
            { adminId, url, sitemapUrl },
            { $set: { crawled: true, crawledAt: new Date() } },
          );
          processedInSession.add(url);
          continue;
        }

        // Check for stop signal
        if (adminId) {
          const db = await getDb();
          const state = await db
            .collection("crawl_states")
            .findOne({ adminId });
          if (state?.status === "stopped") {
            // console.log removed
            break;
          }
        }

        // Check if we're approaching the timeout limit
        const currentTime = Date.now();
        const elapsedTime = currentTime - startTime;

        if (elapsedTime > MAX_EXECUTION_TIME) {
          // console.log removed
          // console.log removed
          timeoutReached = true;
          break;
        }

        // Also check if we're close to timeout and have processed some URLs
        if (elapsedTime > MAX_EXECUTION_TIME - 30000 && crawlCount > 0) {
          // console.log removed
          // console.log removed
          timeoutReached = true;
          break;
        }

        crawlCount++;
        try {
          // console.log removed
          const crawlStartTime = Date.now();

          let text = await extractTextFromUrl(url);
          const rawBlocks = parseSectionBlocks(text);
          const minChars = rawBlocks.length >= 8 ? 30 : 100;
          const mergedBlocks =
            rawBlocks.length > 0
              ? mergeSmallSectionBlocks(rawBlocks, minChars)
              : rawBlocks;
          if (mergedBlocks.length > 0) {
            text = blocksToSectionedText(mergedBlocks);
          }
          const endTime = Date.now();

          // console.log removed
          // console.log removed

          // Debug: Log if text is too short
          if (text.length < 50) {
            // console.log removed
          }

          // console.log removed
          results.push({ url, text });

          // Generate structured summary using shared library
          let structuredSummary: any = null;
          let basicSummary = "Summary not available";

          try {
            structuredSummary = await generateStructuredSummaryFromText(text);
            if (structuredSummary) {
              structuredSummary = normalizeStructuredSummary(structuredSummary);
              if (
                structuredSummary.sections &&
                structuredSummary.sections.length > 0
              ) {
                basicSummary =
                  structuredSummary.sections
                    .map((s: any) => s.sectionSummary)
                    .join("\n\n")
                    .slice(0, 500) + "...";
              }
            }
          } catch (e) {
            console.error("Structured summary generation failed:", e);
          }

          // Legacy block (disabled)
          if (false) {
            try {
              // console.log removed
              const structuredSummaryResponse =
                await openai.chat.completions.create({
                  model: "gpt-4o-mini",
                  response_format: { type: "json_object" },
                  messages: [
                    {
                      role: "system",
                      content:
                        "You are an expert web page analyzer. Your goal is to deconstruct a web page into its distinct logical sections based on the provided [SECTION N] markers and extract key business intelligence for EACH section.\n\nFor EACH section detected, generate:\n1. A Section Title (inferred from content).\n2. EXACTLY TWO HIGHLY SPECIFIC Lead Questions (Problem Recognition) with options mapping to customer states/risks.\n3. EXACTLY TWO HIGHLY SPECIFIC Sales Questions (Diagnostic) with options mapping to root causes.\n4. For each Sales Question, generate a specific 'Option Flow' for EACH option, containing a Diagnostic Answer, Follow-Up Question, Feature Mapping, and Loop Closure.\n\nREQUIREMENTS:\n- Questions MUST be directly derived from the specific content of this section (e.g., if section is 'Lead Scoring', ask about lead prioritization).\n- Do NOT use generic phrasing like 'What are you interested in?' or 'How can we help?'.\n- Use specific terminology found in the Section Content.\n\nReturn ONLY a valid JSON object. Do not include markdown.",
                    },
                    {
                      role: "user",
                      content: `Analyze this web page content:

${text}

Extract and return a JSON object with this exact structure:
{
  "pageType": "homepage|pricing|features|about|contact|blog|product|service",
  "businessVertical": "fitness|healthcare|legal|restaurant|saas|ecommerce|consulting|other",
  "primaryFeatures": ["feature1", "feature2", "feature3"],
  "painPointsAddressed": ["pain1", "pain2", "pain3"],
  "solutions": ["solution1", "solution2", "solution3"],
  "targetCustomers": ["small business", "enterprise", "startups"],
  "businessOutcomes": ["outcome1", "outcome2"],
  "competitiveAdvantages": ["advantage1", "advantage2"],
  "industryTerms": ["term1", "term2", "term3"],
  "pricePoints": ["free", "$X/month", "enterprise"],
  "integrations": ["tool1", "tool2"],
  "useCases": ["usecase1", "usecase2"],
  "callsToAction": ["Get Started", "Book Demo"],
  "trustSignals": ["testimonial", "certification", "clientcount"],
  "sections": [
    {
      "sectionName": "Inferred Title (e.g., Onboarding Momentum, Renewals)",
      "sectionSummary": "A concise 2-5 line summary of this section's content (AI generated)",
      "leadQuestions": [
        {
          "question": "Problem Recognition Question (e.g., What usually happens after...?)",
          "options": ["Option 1", "Option 2", "Option 3", "Option 4"],
          "tags": ["tag_for_opt1", "tag_for_opt2", "tag_for_opt3", "tag_for_opt4"],
          "workflow": "ask_sales_question|educational_insight|validation"
        },
        {
          "question": "Feature/Keyword Question (e.g., How do you handle [Specific Feature]...?)",
          "options": ["Option 1", "Option 2", "Option 3", "Option 4"],
          "tags": [],
          "workflow": "ask_sales_question|educational_insight|validation"
        }
      ],
      "salesQuestions": [
        {
          "question": "Diagnostic Question (e.g., What prevents...?)",
          "options": ["Cause 1", "Cause 2", "Cause 3", "Cause 4"],
          "tags": ["cause_tag_1", "cause_tag_2", "cause_tag_3", "cause_tag_4"],
          "workflow": "diagnostic_response",
          "optionFlows": [
            {
              "forOption": "Cause 1",
              "diagnosticAnswer": "Empathic reflection and validation of the problem.",
              "followUpQuestion": "Specific follow-up to narrow down context.",
              "featureMappingAnswer": "Explanation of ONE specific feature that solves this cause.",
              "loopClosure": "Summary statement closing the loop."
            }
          ]
        },
        {
          "question": "Feature/Capability Question (e.g., How does [Capability] impact...?)",
          "options": ["Cause 1", "Cause 2", "Cause 3", "Cause 4"],
          "tags": [],
          "workflow": "diagnostic_response",
          "optionFlows": []
        }
      ]
    }
  ]
}

IMPORTANT REQUIREMENTS:
1. Use the [SECTION N] markers to delineate sections.
2. Ensure Lead Questions focus on identifying the user's current state or problem awareness SPECIFIC to that section.
3. Ensure Sales Questions focus on diagnosing the specific root cause of that problem using section terminology.
4. The second Lead/Sales Question MUST focus on a specific feature, capability, or keyword in the section. Do NOT ask about timeline or urgency.
5. The 'optionFlows' array MUST have an entry for every option in the Sales Question.
6. Tags should be snake_case (e.g., 'onboarding_delay').
7. Do NOT leave arrays (primaryFeatures, painPointsAddressed, etc.) empty if information can be inferred. Populate them with at least 3 items each where possible.
8. Avoid generic questions like "What are you looking for?". Instead ask "How do you currently handle [Section Topic]?".
`,
                    },
                  ],
                  max_tokens: 12000,
                  temperature: 0.3,
                });

              const structuredText =
                structuredSummaryResponse.choices[0]?.message?.content;
              if (structuredText) {
                try {
                  const parsed = JSON.parse(structuredText || "{}");
                  structuredSummary = normalizeStructuredSummary(parsed);
                  try {
                    let blocks = parseSectionBlocks(text || "");
                    const minChars = blocks.length >= 8 ? 30 : 100;
                    blocks =
                      Array.isArray(blocks) && blocks.length > 0
                        ? mergeSmallSectionBlocks(blocks, minChars)
                        : blocks;
                    if (Array.isArray(structuredSummary?.sections)) {
                      if (
                        blocks.length > 0 &&
                        structuredSummary.sections.length !== blocks.length
                      ) {
                        const baseSections = structuredSummary.sections;
                        structuredSummary.sections = blocks.map(
                          (block, idx) => {
                            const base =
                              baseSections[idx] ||
                              baseSections[baseSections.length - 1] ||
                              {};
                            const baseName =
                              typeof base.sectionName === "string"
                                ? base.sectionName
                                : "";
                            const sectionName =
                              block.title || baseName || `Section ${idx + 1}`;
                            const baseSummary =
                              typeof base.sectionSummary === "string"
                                ? base.sectionSummary
                                : "";
                            const trimmedSummary = baseSummary.trim();
                            const summary =
                              trimmedSummary.length > 0
                                ? trimmedSummary
                                : (() => {
                                    const body = block.body || "";
                                    if (!body) return sectionName;
                                    return body.length > 400
                                      ? body.slice(0, 400) + "..."
                                      : body;
                                  })();
                            return {
                              ...base,
                              sectionName,
                              sectionSummary: summary,
                              sectionContent: block.body || "",
                            };
                          },
                        );
                      }
                      structuredSummary.sections = await Promise.all(
                        structuredSummary.sections.map(
                          async (sec: any, idx: number) => {
                            const name = String(
                              sec?.sectionName || `Section ${idx + 1}`,
                            );
                            const summary = String(sec?.sectionSummary || "");
                            const block = blocks.find(
                              (b) =>
                                b.title &&
                                name &&
                                b.title.toLowerCase() === name.toLowerCase(),
                            ) ||
                              blocks[idx] || { title: name, body: summary };
                            const lead = Array.isArray(sec.leadQuestions)
                              ? sec.leadQuestions[0]
                              : null;
                            const sales = Array.isArray(sec.salesQuestions)
                              ? sec.salesQuestions[0]
                              : null;
                            const sectionType = classifySectionType(
                              name,
                              summary,
                              String(block.body || ""),
                            );
                            // STORE RAW CONTENT for accurate chatbot matching
                            // Fallback: If no parsed blocks found (no [SECTION] markers), use full text for the first section
                            if (
                              blocks.length === 0 &&
                              idx === 0 &&
                              text &&
                              text.length > 50
                            ) {
                              // console.log removed
                              sec.sectionContent = text;
                            } else {
                              sec.sectionContent = block.body || "";
                              if (idx === 0) {
                                // console.log removed
                              }
                            }

                            // Generate questions using the master prompt
                            const questionsData = await refineSectionQuestions(
                              openai,
                              url,
                              String(structuredSummary?.pageType || "other"),
                              String(idx + 1),
                              name,
                              String(block.body || ""),
                              summary,
                              sectionType,
                            );

                            if (questionsData) {
                              // Map lead questions
                              sec.leadQuestions = (
                                Array.isArray(questionsData.leadQuestions)
                                  ? questionsData.leadQuestions
                                  : []
                              )
                                .slice(0, 2)
                                .map((lq: any) => {
                                  let opts = Array.isArray(lq?.options)
                                    ? lq.options.map((o: any) =>
                                        String(o?.label || ""),
                                      )
                                    : [];
                                  if (opts.length < 2)
                                    opts = ["Option 1", "Option 2"];
                                  if (opts.length > 4) opts = opts.slice(0, 4);
                                  const tags = Array.isArray(lq?.options)
                                    ? Array.from(
                                        new Set(
                                          lq.options
                                            .flatMap((o: any) =>
                                              Array.isArray(o?.tags)
                                                ? o.tags
                                                : [],
                                            )
                                            .map((t: any) =>
                                              snakeTag(String(t)),
                                            ),
                                        ),
                                      )
                                    : [];
                                  return {
                                    question: String(lq?.question || ""),
                                    options: opts,
                                    tags,
                                    workflow:
                                      typeof lq?.workflow === "string"
                                        ? lq.workflow
                                        : "ask_sales_question",
                                  };
                                });
                              // Map sales questions
                              sec.salesQuestions = (
                                Array.isArray(questionsData.salesQuestions)
                                  ? questionsData.salesQuestions
                                  : []
                              )
                                .slice(0, 2)
                                .map((sq: any) => {
                                  let opts = Array.isArray(sq?.options)
                                    ? sq.options.map((o: any) =>
                                        String(o?.label || ""),
                                      )
                                    : [];
                                  if (opts.length < 2)
                                    opts = ["Option 1", "Option 2"];
                                  if (opts.length > 4) opts = opts.slice(0, 4);
                                  const flowsRaw = Array.isArray(sq?.options)
                                    ? sq.options
                                    : [];
                                  const ensuredFlows = opts.map(
                                    (label: string) => {
                                      const match =
                                        flowsRaw.find(
                                          (o: any) =>
                                            String(o?.label || "") === label,
                                        ) || {};
                                      return {
                                        forOption: label,
                                        diagnosticAnswer: "",
                                        followUpQuestion: "",
                                        followUpOptions: [],
                                        featureMappingAnswer: "",
                                        loopClosure: "",
                                      };
                                    },
                                  );
                                  const tags = Array.isArray(sq?.options)
                                    ? Array.from(
                                        new Set(
                                          sq.options
                                            .flatMap((o: any) =>
                                              Array.isArray(o?.tags)
                                                ? o.tags
                                                : [],
                                            )
                                            .map((t: any) =>
                                              snakeTag(String(t)),
                                            ),
                                        ),
                                      )
                                    : [];
                                  return {
                                    question: String(sq?.question || ""),
                                    options: opts,
                                    tags,
                                    workflow: "diagnostic_response",
                                    optionFlows: ensuredFlows,
                                  };
                                });

                              // Apply standalone tag generation
                              try {
                                sec.leadQuestions =
                                  await processQuestionsWithTags(
                                    sec.leadQuestions,
                                    block.body,
                                    adminId || undefined,
                                    structuredSummary?.businessName,
                                  );
                                sec.salesQuestions =
                                  await processQuestionsWithTags(
                                    sec.salesQuestions,
                                    block.body,
                                    adminId || undefined,
                                    structuredSummary?.businessName,
                                  );
                              } catch (e) {
                                console.error(
                                  `[Crawl] Tag generation failed for ${url} section ${idx}`,
                                  e,
                                );
                              }
                            }
                            return sec;
                          },
                        ),
                      );
                    }
                    structuredSummary =
                      normalizeStructuredSummary(structuredSummary);
                  } catch {}
                  // console.log removed
                } catch (parseError) {
                  // console.error removed
                }
              }
            } catch (summaryError) {
              // console.error removed
            }
          }
          if (!structuredSummary && text && text.trim().length > 0) {
            const fallback = buildFallbackStructuredSummaryFromText(text);
            if (fallback) {
              structuredSummary = normalizeStructuredSummary(fallback);
              try {
                let blocks = parseSectionBlocks(text || "");
                const minChars = blocks.length >= 8 ? 30 : 100;
                blocks =
                  Array.isArray(blocks) && blocks.length > 0
                    ? mergeSmallSectionBlocks(blocks, minChars)
                    : blocks;
                if (Array.isArray(structuredSummary?.sections)) {
                  structuredSummary.sections = await Promise.all(
                    structuredSummary.sections.map(
                      async (sec: any, idx: number) => {
                        const name = String(
                          sec?.sectionName || `Section ${idx + 1}`,
                        );
                        const summary = String(sec?.sectionSummary || "");
                        const block = blocks.find(
                          (b) =>
                            b.title &&
                            name &&
                            b.title.toLowerCase() === name.toLowerCase(),
                        ) ||
                          blocks[idx] || { title: name, body: summary };
                        const sectionType = classifySectionType(
                          name,
                          summary,
                          String(block.body || ""),
                        );
                        // STORE RAW CONTENT for accurate chatbot matching
                        // Fallback: If no parsed blocks found (no [SECTION] markers), use full text for the first section
                        if (
                          blocks.length === 0 &&
                          idx === 0 &&
                          text &&
                          text.length > 50
                        ) {
                          // console.log removed
                          sec.sectionContent = text;
                        } else {
                          sec.sectionContent = block.body || "";
                          if (idx === 0) {
                            // console.log removed
                          }
                        }

                        const questionsData = await refineSectionQuestions(
                          openai,
                          url,
                          String(structuredSummary?.pageType || "other"),
                          String(idx + 1),
                          name,
                          String(block.body || ""),
                          summary,
                          sectionType,
                        );

                        if (questionsData) {
                          // Lead
                          sec.leadQuestions = (
                            Array.isArray(questionsData.leadQuestions)
                              ? questionsData.leadQuestions
                              : []
                          )
                            .slice(0, 2)
                            .map((lq: any) => {
                              let opts = Array.isArray(lq?.options)
                                ? lq.options
                                : [];
                              opts = opts.map((o: any) => ({
                                label: String(o?.label || ""),
                                tags: Array.isArray(o?.tags)
                                  ? o.tags.map((t: any) => snakeTag(String(t)))
                                  : [],
                                workflow:
                                  typeof o?.workflow === "string"
                                    ? o.workflow
                                    : "education_path",
                              }));
                              if (opts.length < 2) {
                                while (opts.length < 2) {
                                  opts.push({
                                    label: `Option ${opts.length + 1}`,
                                    tags: [],
                                    workflow: "education_path",
                                  });
                                }
                              }
                              if (opts.length > 4) opts = opts.slice(0, 4);
                              return {
                                question: String(lq?.question || ""),
                                options: opts,
                                tags: [],
                                workflow: "validation_path",
                              };
                            });
                          // Sales
                          sec.salesQuestions = (
                            Array.isArray(questionsData.salesQuestions)
                              ? questionsData.salesQuestions
                              : []
                          )
                            .slice(0, 2)
                            .map((sq: any) => {
                              let opts = Array.isArray(sq?.options)
                                ? sq.options
                                : [];
                              opts = opts.map((o: any) => ({
                                label: String(o?.label || ""),
                                tags: Array.isArray(o?.tags)
                                  ? o.tags.map((t: any) => snakeTag(String(t)))
                                  : [],
                                workflow:
                                  typeof o?.workflow === "string"
                                    ? o.workflow
                                    : "optimization_workflow",
                              }));
                              if (opts.length < 2) {
                                while (opts.length < 2) {
                                  opts.push({
                                    label: `Option ${opts.length + 1}`,
                                    tags: [],
                                    workflow: "optimization_workflow",
                                  });
                                }
                              }
                              if (opts.length > 4) opts = opts.slice(0, 4);

                              const ensuredFlows = opts.map((o: any) => ({
                                forOption: o.label,
                                diagnosticAnswer: "",
                                followUpQuestion: "",
                                followUpOptions: [],
                                featureMappingAnswer: "",
                                loopClosure: "",
                              }));

                              return {
                                question: String(sq?.question || ""),
                                options: opts,
                                tags: [],
                                workflow: "diagnostic_education",
                                optionFlows: ensuredFlows,
                              };
                            });

                          // Apply standalone tag generation
                          try {
                            sec.leadQuestions = await processQuestionsWithTags(
                              sec.leadQuestions,
                              block.body,
                              adminId || undefined,
                              structuredSummary?.businessName,
                            );
                            sec.salesQuestions = await processQuestionsWithTags(
                              sec.salesQuestions,
                              block.body,
                              adminId || undefined,
                              structuredSummary?.businessName,
                            );
                          } catch (e) {
                            // console.error removed
                          }
                        }
                        return sec;
                      },
                    ),
                  );
                }
              } catch {}
              // console.log removed
            }
          }

          // Store page data with both summaries
          const pageData: any = {
            adminId,
            url,
            text,
            summary: basicSummary,
            filename: url,
            createdAt: new Date(),
          };

          // Add structured summary if generated
          if (structuredSummary) {
            pageData.summaryGeneratedAt = new Date();
          }

          // Use upsert to prevent duplicate entries for the same URL
          await pages.updateOne(
            { adminId, url },
            { $set: pageData },
            { upsert: true },
          );
          if (structuredSummary) {
            const dbRef = await getDb();
            const structuredSummaries = dbRef.collection(
              "structured_summaries",
            );
            const pageDoc = await pages.findOne({ adminId, url });
            if (pageDoc && pageDoc._id) {
              const pageId = pageDoc._id;

              // Explicitly delete old structured summary to ensure fresh start and no stale data
              await structuredSummaries.deleteOne({ adminId, pageId });

              await structuredSummaries.insertOne({
                adminId,
                pageId,
                url,
                structuredSummary,
                summaryGeneratedAt: new Date(),
              });
            }
          }
          // console.log removed

          // Mark as crawled in sitemap_urls with specific sitemapUrl context
          // console.log removed
          await sitemapUrls.updateOne(
            { adminId, url, sitemapUrl }, // Include sitemapUrl to ensure proper tracking
            { $set: { crawled: true, crawledAt: new Date() } },
          );
          // Chunk and embed for Pinecone
          let chunks = chunkText(text);
          // console.log removed

          // Debug: If no chunks created, log why and try to create a minimal chunk
          if (chunks.length === 0) {
            // console.log removed

            // If we have some text but no chunks, create a minimal chunk
            if (text.length > 10) {
              // console.log removed
              chunks = [text.trim()];
            }
          }

          if (chunks.length > 0) {
            // console.log removed
            try {
              const embedResp = await openai.embeddings.create({
                input: chunks,
                model: "text-embedding-3-small",
                dimensions: 1024,
              });
              const embeddings = embedResp.data.map(
                (d: { embedding: number[] }) => d.embedding,
              );
              // console.log removed

              const metadata = chunks.map((chunk, i) => ({
                filename: url,
                adminId,
                url,
                chunkIndex: i,
              }));

              // Clean up old vectors for this URL before adding new ones
              // console.log removed
              await deleteChunksByUrl(url, adminId);

              // console.log removed
              await addChunks(chunks, embeddings, metadata);
              totalChunks += chunks.length;
              // console.log removed
            } catch (embeddingError) {
              // console.error removed
              if (embeddingError instanceof Error) {
                // console.error removed
              }
            }
          } else {
            // console.log removed
          }
        } catch (err) {
          // console.error removed
          if (err instanceof Error) {
            // console.error removed
          }

          // Mark as failed in sitemap_urls (but don't set crawled to true)
          await sitemapUrls.updateOne(
            { adminId, url, sitemapUrl }, // Include sitemapUrl for proper tracking
            {
              $set: {
                failedAt: new Date(),
                error: err instanceof Error ? err.message : String(err),
              },
            },
          );
          failedUrls.push({
            url,
            error: err instanceof Error ? err.message : String(err),
          });
        }
        processedInSession.add(url);
      }

      if (timeoutReached) {
        break;
      }

      if (adminId) {
        const db = await getDb();
        const state = await db.collection("crawl_states").findOne({ adminId });
        if (state?.status === "stopped") {
          // console.log removed
          break;
        }
      }
    }

    const totalElapsedTime = Date.now() - startTime;

    // Recalculate remaining based on DB truth
    const finalCrawledCount = await sitemapUrls.countDocuments({
      adminId,
      sitemapUrl,
      crawled: true,
    });
    const totalRemaining = Math.max(0, urls.length - finalCrawledCount);
    const hasMorePages = totalRemaining > 0;

    // console.log removed
    // console.log removed
    // console.log removed

    if (timeoutReached) {
      // console.log removed
    }

    const response = {
      crawled: results.length,
      totalChunks,
      pages: results.map((r) => r.url),
      failedPages: failedUrls,
      batchDone: results.length, // Number of pages successfully crawled in this batch
      batchRemaining: totalRemaining, // Total remaining pages
      totalRemaining: totalRemaining,
      recrawledPages: problematicUrls.length, // Show how many pages were reset for re-crawling
      timeoutReached, // Indicate if processing stopped due to timeout
      executionTime: totalElapsedTime,
      totalDiscovered: urls.length,
      hasMorePages, // Indicates if there are more pages to crawl
      sitemapUrl, // Include the sitemap URL for auto-continue
      message: timeoutReached
        ? `Processed ${results.length} pages before timeout. ${totalRemaining} pages remaining.`
        : hasMorePages
          ? `Successfully processed ${results.length} pages. ${totalRemaining} pages remaining - auto-continue available.`
          : `All ${urls.length} pages have been successfully processed!`,
    };

    // Auto-extract personas when crawling is complete
    // Check if stopped
    let isStopped = false;
    if (adminId) {
      const db = await getDb();
      const state = await db.collection("crawl_states").findOne({ adminId });
      isStopped = state?.status === "stopped";
    }

    if (
      (!hasMorePages && results.length > 0) ||
      (isStopped && finalCrawledCount > 0)
    ) {
      // console.log removed
      try {
        await extractPersonasForAdmin(adminId, normalizedUrl);
        await autoGenerateBantConfig(adminId);
        await autoGenerateDiagnosticAnswers(adminId);
        // console.log removed
      } catch (personaError) {
        // console.error removed
        // Don't fail the main response if persona extraction fails
      }
    }

    // console.log removed
    return NextResponse.json(response);
  } catch (error) {
    // console.error removed
    if (error instanceof Error) {
      // console.error removed
    }

    const errorResponse = {
      error:
        "An error occurred while processing the sitemap. Please try again.",
      details: error instanceof Error ? error.message : String(error),
      timestamp: new Date().toISOString(),
    };

    // console.log removed
    return NextResponse.json(errorResponse, { status: 500 });
  }
}

export async function GET(req: NextRequest) {
  // If ?debug=1 with API key, return sitemap debug info
  if (req.nextUrl.searchParams.get("debug") === "1") {
    const apiKey = req.headers.get("x-api-key");
    if (!apiKey) {
      return NextResponse.json(
        { error: "API key required for debug" },
        { status: 401 },
      );
    }

    const apiAuth = await verifyApiKey(apiKey);
    if (!apiAuth) {
      return NextResponse.json({ error: "Invalid API key" }, { status: 401 });
    }

    const adminId = apiAuth.adminId;
    const db = await getDb();
    const sitemapUrls = db.collection("sitemap_urls");

    // Get all sitemap entries for this admin
    const entries = await sitemapUrls.find({ adminId }).toArray();

    // Get specific page check if provided
    const checkUrl = req.nextUrl.searchParams.get("url");
    let specificEntry = null;
    if (checkUrl) {
      specificEntry = await sitemapUrls.findOne({ adminId, url: checkUrl });
    }

    return NextResponse.json({
      adminId,
      email: apiAuth.email,
      totalEntries: entries.length,
      entries: entries.map((e) => ({
        url: e.url,
        crawled: e.crawled,
        crawledAt: e.crawledAt,
      })),
      specificUrlCheck: checkUrl
        ? { url: checkUrl, found: !!specificEntry, entry: specificEntry }
        : null,
    });
  }

  // If ?settings=1, return admin settings (last submitted sitemapUrl)
  if (req.nextUrl.searchParams.get("settings") === "1") {
    const token = req.cookies.get("auth_token")?.value;
    if (!token)
      return NextResponse.json({ error: "Unauthorized" }, { status: 401 });
    let adminId = "";
    try {
      const payload = jwt.verify(token, JWT_SECRET) as {
        email: string;
        adminId: string;
      };
      adminId = payload.adminId;
    } catch {
      return NextResponse.json({ error: "Invalid token" }, { status: 401 });
    }
    const adminSettings = await getAdminSettingsCollection();
    const settings = await adminSettings.findOne({ adminId });
    return NextResponse.json({ settings });
  }
  // If ?urls=1, return all sitemap URLs for the current admin
  if (req.nextUrl.searchParams.get("urls") === "1") {
    const token = req.cookies.get("auth_token")?.value;
    if (!token)
      return NextResponse.json({ error: "Unauthorized" }, { status: 401 });
    let adminId = "";
    try {
      const payload = jwt.verify(token, JWT_SECRET) as {
        email: string;
        adminId: string;
      };
      adminId = payload.adminId;
    } catch {
      return NextResponse.json({ error: "Invalid token" }, { status: 401 });
    }
    const db = await getDb();
    const sitemapUrls = db.collection("sitemap_urls");
    const urls = await sitemapUrls
      .find({ adminId })
      .project({
        _id: 0,
        url: 1,
        crawled: 1,
        crawledAt: 1,
        sitemapUrl: 1,
        discoveryType: 1,
      })
      .toArray();
    return NextResponse.json({ urls });
  }
  const token = req.cookies.get("auth_token")?.value;
  if (!token)
    return NextResponse.json({ error: "Unauthorized" }, { status: 401 });
  let adminId = "";
  try {
    const payload = jwt.verify(token, JWT_SECRET) as {
      email: string;
      adminId: string;
    };
    adminId = payload.adminId;
  } catch {
    return NextResponse.json({ error: "Invalid token" }, { status: 401 });
  }

  // Pagination params
  const page = parseInt(req.nextUrl.searchParams.get("page") || "1", 10);
  const pageSize = parseInt(
    req.nextUrl.searchParams.get("pageSize") || "10",
    10,
  );

  const db = await getDb();
  const pages = db.collection("crawled_pages");

  // Aggregate by sitemap (group by sitemapUrl/filename)
  const pipeline = [
    { $match: { adminId } },
    {
      $group: {
        _id: "$filename",
        count: { $sum: 1 },
        firstCrawled: { $min: "$createdAt" },
        urls: { $addToSet: "$url" },
      },
    },
    { $sort: { firstCrawled: -1 } },
    { $skip: (page - 1) * pageSize },
    { $limit: pageSize },
  ];
  const sitemaps = await pages.aggregate(pipeline).toArray();
  const total = await pages.distinct("filename", { adminId });

  return NextResponse.json({
    sitemaps: sitemaps.map((s) => ({
      sitemapUrl: s._id,
      count: s.count,
      firstCrawled: s.firstCrawled,
      urls: s.urls,
    })),
    total: total.length,
    page,
    pageSize,
  });
}

export async function DELETE(req: NextRequest) {
  const token = req.cookies.get("auth_token")?.value;
  if (!token)
    return NextResponse.json({ error: "Unauthorized" }, { status: 401 });
  let adminId = "";
  try {
    const payload = jwt.verify(token, JWT_SECRET) as {
      email: string;
      adminId: string;
    };
    adminId = payload.adminId;
  } catch {
    return NextResponse.json({ error: "Invalid token" }, { status: 401 });
  }

  const { sitemapUrl, url } = await req.json();
  if (!sitemapUrl && !url)
    return NextResponse.json(
      { error: "Missing sitemapUrl or url" },
      { status: 400 },
    );

  const db = await getDb();
  const pages = db.collection("crawled_pages");

  let deleteCount = 0;
  if (sitemapUrl) {
    // Delete all pages for this sitemap
    // First find all pages to get their URLs for chunk deletion
    const pagesToDelete = await pages
      .find({ adminId, filename: sitemapUrl })
      .project({ url: 1 })
      .toArray();

    // Delete chunks for each page found FIRST (Critical for data consistency)
    for (const p of pagesToDelete) {
      if (p.url) {
        await deleteChunksByUrl(p.url, adminId);
      }
    }
    // Also try to delete by filename in case some were stored that way
    await deleteChunksByFilename(sitemapUrl, adminId);

    // Then delete the pages from MongoDB
    const result = await pages.deleteMany({ adminId, filename: sitemapUrl });
    deleteCount = result.deletedCount || 0;

    // Also delete from sitemap_urls to allow re-crawling
    const sitemapUrls = db.collection("sitemap_urls");
    await sitemapUrls.deleteMany({ adminId, sitemapUrl });
  } else if (url) {
    // Delete a single page
    // Delete vectors FIRST
    await deleteChunksByUrl(url, adminId);

    // Then delete from MongoDB
    const result = await pages.deleteMany({ adminId, url });
    deleteCount = result.deletedCount || 0;

    // Also delete from sitemap_urls to allow re-crawling
    const sitemapUrls = db.collection("sitemap_urls");
    await sitemapUrls.deleteMany({ adminId, url });
  }

  return NextResponse.json({ success: true, deleted: deleteCount });
}

// Logic moved to src/lib/diagnostic.ts

// Helper removed
